{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuWqm37B1Ous"
      },
      "source": [
        "### Install the dependancies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cog4zW-kLIY3",
        "outputId": "1a6d7736-107c-4987-80d3-f5fbf5ebd0b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.23.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.10.6)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.47)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.18)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.23-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.6-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.59-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.27.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.21.0-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.0.0)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading gradio-5.23.1-py3-none-any.whl (51.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.3.20-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.23-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.6-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.59-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.31.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl (31 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.21.0-py2.py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading ormsgpack-1.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=09c545d0bb3fb53b311f6eeecbd2a85e5c447fca241c67c804f2d4e2e45163f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, pydub, monotonic, durationpy, xxhash, uvloop, uvicorn, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, pyproject_hooks, overrides, ormsgpack, opentelemetry-util-http, opentelemetry-proto, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mmh3, humanfriendly, httptools, groovy, ffmpy, chroma-hnswlib, bcrypt, backoff, asgiref, aiofiles, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, build, safehttpx, onnxruntime, nvidia-cusolver-cu12, langgraph-sdk, kubernetes, gradio-client, fastapi, opentelemetry-instrumentation, gradio, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langgraph-checkpoint, opentelemetry-instrumentation-fastapi, langgraph-prebuilt, langgraph, chromadb\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed aiofiles-23.2.1 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.6.3 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.1 gradio-client-1.8.0 groovy-0.1.2 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 langgraph-0.3.20 langgraph-checkpoint-2.0.23 langgraph-prebuilt-0.1.6 langgraph-sdk-0.1.59 mmh3-5.1.0 monotonic-1.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.21.0 opentelemetry-exporter-otlp-proto-common-1.31.1 opentelemetry-exporter-otlp-proto-grpc-1.31.1 opentelemetry-instrumentation-0.52b1 opentelemetry-instrumentation-asgi-0.52b1 opentelemetry-instrumentation-fastapi-0.52b1 opentelemetry-proto-1.31.1 opentelemetry-util-http-0.52b1 ormsgpack-1.9.0 overrides-7.7.0 posthog-3.21.0 pydub-0.25.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.0 python-multipart-0.0.20 ruff-0.11.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio langchain langgraph pydantic transformers chromadb sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vYjc5E1qLiYG"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio langchain langchain-community langgraph>=0.0.20 transformers torch sentence-transformers chromadb networkx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrZmOibT1bEf"
      },
      "source": [
        "### Create directory for source files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tl4e1jDjLkh2"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Create inline source files for the system components\n",
        "\n",
        "# Create directory structure\n",
        "!mkdir -p agentic_rag-mcp_system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxSXYnOwLnR3",
        "outputId": "2f6bf5ed-0bdb-4cac-f46f-cfe15cd94008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/agentic_rag-mcp_system\n"
          ]
        }
      ],
      "source": [
        "# Change to that directory\n",
        "%cd agentic_rag-mcp_system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5exy75s12Uk"
      },
      "source": [
        "### Model Context Protocol Implementation Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtNpOjWGLnNG",
        "outputId": "4443242d-2616-433e-a5ef-bd72002092ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing full_mcp_implementation.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile full_mcp_implementation.py\n",
        "import os\n",
        "import logging\n",
        "import re\n",
        "import json\n",
        "from typing import Dict, List, Any, Optional, Union\n",
        "from abc import ABC, abstractmethod\n",
        "from pathlib import Path\n",
        "\n",
        "# Configure MCP logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(\"mcp_system\")\n",
        "\n",
        "# Define MCP Framework components\n",
        "# The below code block define Specialized Agents for each task : Device Search, Troubleshooting, Observability, Knowledge Base and Incident Response\n",
        "\n",
        "class AgentContext:\n",
        "    \"\"\"Context container for sharing information between agents\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.conversation_history = []\n",
        "        self.retrieved_knowledge = {}\n",
        "        self.entity_memory = {}\n",
        "        self.execution_state = {}\n",
        "        self.last_agent = None\n",
        "\n",
        "    def add_message(self, role: str, content: str):\n",
        "        \"\"\"Add a message to the conversation history\"\"\"\n",
        "        self.conversation_history.append({\"role\": role, \"content\": content})\n",
        "\n",
        "    def add_knowledge(self, agent_type: str, knowledge: Dict[str, Any]):\n",
        "        \"\"\"Add retrieved knowledge for a specific agent type\"\"\"\n",
        "        self.retrieved_knowledge[agent_type] = knowledge\n",
        "\n",
        "    def remember_entity(self, entity_type: str, entity_id: str, properties: Dict[str, Any]):\n",
        "        \"\"\"Remember an entity like a device or service\"\"\"\n",
        "        if entity_type not in self.entity_memory:\n",
        "            self.entity_memory[entity_type] = {}\n",
        "        self.entity_memory[entity_type][entity_id] = properties\n",
        "\n",
        "    def get_conversation_summary(self, last_n=5):\n",
        "        \"\"\"Get a summary of the last n conversation turns\"\"\"\n",
        "        return self.conversation_history[-last_n:] if self.conversation_history else []\n",
        "\n",
        "    def update_state(self, key: str, value: Any):\n",
        "        \"\"\"Update execution state\"\"\"\n",
        "        self.execution_state[key] = value\n",
        "\n",
        "    def get_state(self, key: str, default=None):\n",
        "        \"\"\"Get value from execution state\"\"\"\n",
        "        return self.execution_state.get(key, default)\n",
        "\n",
        "    def set_last_agent(self, agent_type: str):\n",
        "        \"\"\"Set the last agent that processed a request\"\"\"\n",
        "        self.last_agent = agent_type\n",
        "\n",
        "    def get_last_agent(self):\n",
        "        \"\"\"Get the last agent that processed a request\"\"\"\n",
        "        return self.last_agent\n",
        "\n",
        "class BaseAgent(ABC):\n",
        "    \"\"\"Base class for all MCP agents\"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: str, agent_type: str, capabilities: List[str]):\n",
        "        self.agent_id = agent_id\n",
        "        self.agent_type = agent_type\n",
        "        self.capabilities = capabilities\n",
        "        self.rag_system = None\n",
        "\n",
        "    def set_rag_system(self, rag_system):\n",
        "        \"\"\"Set the RAG system to use for knowledge retrieval\"\"\"\n",
        "        self.rag_system = rag_system\n",
        "\n",
        "    @abstractmethod\n",
        "    def process(self, query: str, context: AgentContext) -> Dict[str, Any]:\n",
        "        \"\"\"Process a query and return a response\"\"\"\n",
        "        pass\n",
        "\n",
        "    def can_handle(self, capability: str) -> bool:\n",
        "        \"\"\"Check if this agent can handle a specific capability\"\"\"\n",
        "        return capability in self.capabilities\n",
        "\n",
        "    def get_metadata(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get agent metadata\"\"\"\n",
        "        return {\n",
        "            \"agent_id\": self.agent_id,\n",
        "            \"agent_type\": self.agent_type,\n",
        "            \"capabilities\": self.capabilities\n",
        "        }\n",
        "\n",
        "class TroubleshootingAgent(BaseAgent):\n",
        "    \"\"\"Agent specialized in network troubleshooting\"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: str):\n",
        "        capabilities = [\n",
        "            \"diagnose_network_issues\",\n",
        "            \"interpret_error_messages\",\n",
        "            \"suggest_resolution_steps\"\n",
        "        ]\n",
        "        super().__init__(agent_id, \"troubleshooting\", capabilities)\n",
        "\n",
        "    def process(self, query: str, context: AgentContext) -> Dict[str, Any]:\n",
        "        \"\"\"Process a troubleshooting query\"\"\"\n",
        "        logger.info(f\"TroubleshootingAgent processing query: {query}\")\n",
        "\n",
        "        # Use RAG system to retrieve relevant knowledge\n",
        "        if self.rag_system:\n",
        "            rag_result = self.rag_system.query(\"troubleshooting\", query)\n",
        "            context.add_knowledge(\"troubleshooting\", rag_result)\n",
        "\n",
        "            # Remember any devices mentioned in the query\n",
        "            if \"device\" in context.entity_memory:\n",
        "                for device_id, device in context.entity_memory[\"device\"].items():\n",
        "                    if device_id.lower() in query.lower() or device.get(\"name\", \"\").lower() in query.lower():\n",
        "                        logger.info(f\"Found referenced device in query: {device_id}\")\n",
        "                        context.update_state(\"referenced_device\", device_id)\n",
        "\n",
        "            # Create response\n",
        "            response = {\n",
        "                \"analysis\": rag_result[\"response\"],\n",
        "                \"sources\": rag_result[\"sources\"],\n",
        "                \"suggested_actions\": self._extract_actions(rag_result[\"response\"])\n",
        "            }\n",
        "\n",
        "            # Update context\n",
        "            context.add_message(\"agent\", json.dumps(response))\n",
        "            context.set_last_agent(\"troubleshooting\")\n",
        "\n",
        "            return response\n",
        "        else:\n",
        "            return {\"error\": \"RAG system not available\"}\n",
        "\n",
        "    def _extract_actions(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract suggested actions from the response text\"\"\"\n",
        "        actions = []\n",
        "\n",
        "        # Simple extraction - look for numbered lists and bullets\n",
        "        lines = text.split('\\n')\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            # Match patterns like \"1. Check X\" or \"- Verify Y\"\n",
        "            if (line.startswith('- ') or line.startswith('* ') or\n",
        "                (len(line) > 2 and line[0].isdigit() and line[1] == '.')):\n",
        "                action = line[2:].strip() if line[1] in ['.', ' '] else line[1:].strip()\n",
        "                actions.append(action)\n",
        "\n",
        "        return actions\n",
        "\n",
        "class DeviceSearchAgent(BaseAgent):\n",
        "    \"\"\"Agent specialized in finding network devices\"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: str, langgraph_agent=None):\n",
        "        capabilities = [\n",
        "            \"find_devices\",\n",
        "            \"analyze_topology\",\n",
        "            \"assess_impact\"\n",
        "        ]\n",
        "        super().__init__(agent_id, \"device_search\", capabilities)\n",
        "        # Store reference to LangGraph agent if provided\n",
        "        self.langgraph_agent = langgraph_agent\n",
        "\n",
        "    def process(self, query: str, context: AgentContext) -> Dict[str, Any]:\n",
        "        \"\"\"Process a device search query\"\"\"\n",
        "        logger.info(f\"DeviceSearchAgent processing query: {query}\")\n",
        "\n",
        "        # If LangGraph agent is available, use it\n",
        "        if self.langgraph_agent:\n",
        "            try:\n",
        "                logger.info(\"Using LangGraph agent for device search\")\n",
        "                langgraph_result = self.langgraph_agent(query)\n",
        "\n",
        "                if not langgraph_result.get(\"success\", False):\n",
        "                    error_msg = langgraph_result.get(\"error\", \"Unknown error\")\n",
        "                    logger.error(f\"LangGraph agent error: {error_msg}\")\n",
        "                    return {\"error\": error_msg}\n",
        "\n",
        "                # Extract found devices\n",
        "                found_devices = langgraph_result.get(\"found_devices\", [])\n",
        "\n",
        "                # Remember devices in context\n",
        "                for device in found_devices:\n",
        "                    device_id = device.get(\"ci_id\", \"\")\n",
        "                    if device_id:\n",
        "                        context.remember_entity(\"device\", device_id, {\n",
        "                            \"id\": device_id,\n",
        "                            \"name\": device.get(\"name\", \"\"),\n",
        "                            \"type\": device.get(\"ci_type\", \"\"),\n",
        "                            \"status\": device.get(\"status\", \"\"),\n",
        "                            \"location\": device.get(\"location\", \"\"),\n",
        "                            \"importance\": device.get(\"importance\", \"\")\n",
        "                        })\n",
        "\n",
        "                # Create response using LangGraph results\n",
        "                response = {\n",
        "                    \"devices\": found_devices,\n",
        "                    \"upstream_devices\": langgraph_result.get(\"upstream_devices\", {}),\n",
        "                    \"downstream_devices\": langgraph_result.get(\"downstream_devices\", {}),\n",
        "                    \"affected_services\": langgraph_result.get(\"affected_services\", {})\n",
        "                }\n",
        "\n",
        "                # Update context\n",
        "                context.add_message(\"agent\", json.dumps(response))\n",
        "                context.set_last_agent(\"device_search\")\n",
        "\n",
        "                return response\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error using LangGraph agent: {e}\")\n",
        "                # Fall back to RAG-based approach if LangGraph fails\n",
        "\n",
        "        # Use RAG system as fallback or primary approach\n",
        "        if self.rag_system:\n",
        "            # First, get general device knowledge\n",
        "            rag_result = self.rag_system.query(\"device_search\", query)\n",
        "            context.add_knowledge(\"device_search\", rag_result)\n",
        "\n",
        "            # Try to find specific devices in inventory if available\n",
        "            device_inventory = None\n",
        "            try:\n",
        "                inventory_result = self.rag_system.query(\"device_inventory\", query)\n",
        "                if inventory_result and \"retrieved_content\" in inventory_result:\n",
        "                    device_inventory = inventory_result\n",
        "                    context.add_knowledge(\"device_inventory\", inventory_result)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error retrieving device inventory: {e}\")\n",
        "\n",
        "            # Extract and remember devices\n",
        "            devices = self._extract_devices(rag_result[\"response\"], device_inventory)\n",
        "            for device in devices:\n",
        "                context.remember_entity(\"device\", device[\"id\"], device)\n",
        "\n",
        "            # Create response\n",
        "            response = {\n",
        "                \"devices\": devices,\n",
        "                \"topology_analysis\": self._analyze_topology(devices, context),\n",
        "                \"sources\": rag_result[\"sources\"]\n",
        "            }\n",
        "\n",
        "            # Update context\n",
        "            context.add_message(\"agent\", json.dumps(response))\n",
        "            context.set_last_agent(\"device_search\")\n",
        "\n",
        "            return response\n",
        "        else:\n",
        "            return {\"error\": \"Neither LangGraph nor RAG system available\"}\n",
        "\n",
        "    def _extract_devices(self, text: str, inventory=None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Extract device information from response text and/or inventory\"\"\"\n",
        "        devices = []\n",
        "\n",
        "        # First try to extract from inventory if available\n",
        "        if inventory and \"response\" in inventory:\n",
        "            try:\n",
        "                # Look for patterns like \"Device ID: xxx \" or similar structured datausing regex\n",
        "                import re\n",
        "                device_pattern = r\"Device ID: (\\w+)[\\s\\n]+Name: ([^\\n]+)[\\s\\n]+Type: ([^\\n]+)[\\s\\n]+Status: ([^\\n]+)\"\n",
        "                matches = re.findall(device_pattern, inventory[\"response\"], re.IGNORECASE)\n",
        "\n",
        "                for match in matches:\n",
        "                    device_id, name, device_type, status = match\n",
        "                    devices.append({\n",
        "                        \"id\": device_id.strip(),\n",
        "                        \"name\": name.strip(),\n",
        "                        \"type\": device_type.strip(),\n",
        "                        \"status\": status.strip()\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error extracting devices from inventory: {e}\")\n",
        "\n",
        "        # If no devices extracted from inventory, try from general response\n",
        "        if not devices:\n",
        "            # Simple extraction - look for device mentions\n",
        "            lines = text.split('\\n')\n",
        "            current_device = {}\n",
        "\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "\n",
        "                # Try to identify device ID or name\n",
        "                if \":\" in line:\n",
        "                    key, value = line.split(\":\", 1)\n",
        "                    key = key.strip().lower()\n",
        "                    value = value.strip()\n",
        "\n",
        "                    if key in [\"device\", \"device id\", \"id\", \"name\"]:\n",
        "                        # If we were tracking a device, save it before starting a new one\n",
        "                        if current_device and \"id\" in current_device:\n",
        "                            devices.append(current_device)\n",
        "                            current_device = {}\n",
        "\n",
        "                        if key in [\"device id\", \"id\"]:\n",
        "                            current_device[\"id\"] = value\n",
        "                        else:  # name\n",
        "                            current_device[\"name\"] = value\n",
        "                            if \"id\" not in current_device:\n",
        "                                # Generate an ID if none exists\n",
        "                                current_device[\"id\"] = f\"DEV{len(devices):03d}\"\n",
        "\n",
        "                    # Capture other device attributes\n",
        "                    elif key in [\"type\", \"status\", \"location\", \"importance\"] and current_device:\n",
        "                        current_device[key] = value\n",
        "\n",
        "            # Add the last device for tracking purposes\n",
        "            if current_device and \"id\" in current_device:\n",
        "                devices.append(current_device)\n",
        "\n",
        "        return devices\n",
        "\n",
        "    def _analyze_topology(self, devices: List[Dict[str, Any]], context: AgentContext) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze topology relationships between devices\"\"\"\n",
        "        # Code block to draw on knowledge about network topology\n",
        "        # For this example, we'll use a simplified approach\n",
        "\n",
        "        topology = {\n",
        "            \"connections\": [],\n",
        "            \"dependencies\": [],\n",
        "            \"critical_paths\": []\n",
        "        }\n",
        "\n",
        "        # Identify potential connections based on device types\n",
        "        for i, device1 in enumerate(devices):\n",
        "            for j, device2 in enumerate(devices):\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                # Simple topology rules\n",
        "                if device1.get(\"type\", \"\").lower() == \"router\" and device2.get(\"type\", \"\").lower() == \"switch\":\n",
        "                    topology[\"connections\"].append({\n",
        "                        \"from\": device1[\"id\"],\n",
        "                        \"to\": device2[\"id\"],\n",
        "                        \"type\": \"uplink\"\n",
        "                    })\n",
        "                    topology[\"dependencies\"].append({\n",
        "                        \"dependent\": device2[\"id\"],\n",
        "                        \"depends_on\": device1[\"id\"],\n",
        "                        \"reason\": \"Routing dependency\"\n",
        "                    })\n",
        "\n",
        "        # Check for critical paths\n",
        "        for conn in topology[\"connections\"]:\n",
        "            from_device = next((d for d in devices if d[\"id\"] == conn[\"from\"]), None)\n",
        "            if from_device and from_device.get(\"importance\", \"\").lower() in [\"critical\", \"high\"]:\n",
        "                topology[\"critical_paths\"].append({\n",
        "                    \"path_id\": f\"PATH{len(topology['critical_paths']):03d}\",\n",
        "                    \"devices\": [conn[\"from\"], conn[\"to\"]],\n",
        "                    \"importance\": from_device.get(\"importance\", \"unknown\")\n",
        "                })\n",
        "\n",
        "        return topology\n",
        "\n",
        "class KnowledgeBaseAgent(BaseAgent):\n",
        "    \"\"\"Agent specialized in retrieving knowledge base information\"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: str):\n",
        "        capabilities = [\n",
        "            \"answer_questions\",\n",
        "            \"provide_references\",\n",
        "            \"explain_concepts\"\n",
        "        ]\n",
        "        super().__init__(agent_id, \"knowledge_base\", capabilities)\n",
        "\n",
        "    def process(self, query: str, context: AgentContext) -> Dict[str, Any]:\n",
        "        \"\"\"Process a knowledge base query\"\"\"\n",
        "        logger.info(f\"KnowledgeBaseAgent processing query: {query}\")\n",
        "\n",
        "        # Parse document type if specified\n",
        "        doc_type = None\n",
        "        if \"document type:\" in query.lower():\n",
        "            parts = query.split(\"document type:\", 1)\n",
        "            query_text = parts[0].strip()\n",
        "            doc_type = parts[1].strip()\n",
        "            logger.info(f\"Detected document type filter: {doc_type}\")\n",
        "        else:\n",
        "            query_text = query\n",
        "\n",
        "        # Enrich query with context from conversation\n",
        "        enriched_query = self._enrich_query(query_text, context)\n",
        "        if doc_type:\n",
        "            enriched_query = f\"{enriched_query} (document type: {doc_type})\"\n",
        "\n",
        "        # Use RAG system to retrieve knowledge\n",
        "        if self.rag_system:\n",
        "            rag_result = self.rag_system.query(\"knowledge_base\", enriched_query)\n",
        "            context.add_knowledge(\"knowledge_base\", rag_result)\n",
        "\n",
        "            # Create response\n",
        "            response = {\n",
        "                \"answer\": rag_result[\"response\"],\n",
        "                \"sources\": rag_result[\"sources\"],\n",
        "                \"related_topics\": self._extract_related_topics(rag_result[\"response\"], rag_result.get(\"retrieved_content\", []))\n",
        "            }\n",
        "\n",
        "            # Update context\n",
        "            context.add_message(\"agent\", json.dumps(response))\n",
        "            context.set_last_agent(\"knowledge_base\")\n",
        "\n",
        "            return response\n",
        "        else:\n",
        "            return {\"error\": \"RAG system not available\"}\n",
        "\n",
        "    def _enrich_query(self, query: str, context: AgentContext) -> str:\n",
        "        \"\"\"Enrich the query with conversational context\"\"\"\n",
        "        enriched = query\n",
        "\n",
        "        # Add device context if available\n",
        "        referenced_device_id = context.get_state(\"referenced_device\")\n",
        "        if referenced_device_id and \"device\" in context.entity_memory:\n",
        "            device = context.entity_memory[\"device\"].get(referenced_device_id)\n",
        "            if device:\n",
        "                device_info = f\" (regarding {device.get('type', 'device')} {device.get('name', referenced_device_id)})\"\n",
        "                enriched += device_info\n",
        "\n",
        "        # Add context from last few conversation rounds\n",
        "        last_msgs = context.get_conversation_summary(last_n=2)\n",
        "        if last_msgs:\n",
        "            # Extract key information from recent messages\n",
        "            recent_context = \" \".join([msg[\"content\"] for msg in last_msgs if msg[\"role\"] == \"user\"])\n",
        "            if recent_context and len(recent_context) > 0:\n",
        "                enriched = f\"{enriched} (context: {recent_context[:100]}...)\"\n",
        "\n",
        "        return enriched\n",
        "\n",
        "    def _extract_related_topics(self, response: str, content: List[str]) -> List[str]:\n",
        "        \"\"\"Extract related topics from the response and retrieved content\"\"\"\n",
        "        topics = set()\n",
        "\n",
        "        # Look for common technical terms\n",
        "        tech_terms = [\n",
        "            \"router\", \"switch\", \"firewall\", \"VPN\", \"ACL\", \"QoS\", \"VLAN\",\n",
        "            \"routing\", \"switching\", \"security\", \"performance\", \"latency\",\n",
        "            \"DNS\", \"DHCP\", \"BGP\", \"OSPF\", \"spanning tree\", \"NAT\"\n",
        "        ]\n",
        "\n",
        "        # Extract terms from response\n",
        "        for term in tech_terms:\n",
        "            if term.lower() in response.lower():\n",
        "                topics.add(term)\n",
        "\n",
        "        # Extract from retrieved content\n",
        "        for doc in content:\n",
        "            for term in tech_terms:\n",
        "                if term.lower() in doc.lower() and term not in topics:\n",
        "                    # Only add a few more to avoid overload\n",
        "                    if len(topics) < 10:\n",
        "                        topics.add(term)\n",
        "\n",
        "        return list(topics)\n",
        "\n",
        "class ObservabilityAgent(BaseAgent):\n",
        "    \"\"\"Agent specialized in network monitoring and observability\"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: str):\n",
        "        capabilities = [\n",
        "            \"analyze_metrics\",\n",
        "            \"detect_anomalies\",\n",
        "            \"forecast_trends\",\n",
        "            \"health_assessment\"\n",
        "        ]\n",
        "        super().__init__(agent_id, \"observability\", capabilities)\n",
        "\n",
        "    def process(self, query: str, context: AgentContext) -> Dict[str, Any]:\n",
        "        \"\"\"Process an observability query\"\"\"\n",
        "        logger.info(f\"ObservabilityAgent processing query: {query}\")\n",
        "\n",
        "        # Parse metric parameters from query\n",
        "        ci_types, metrics, time_range = self._parse_metrics_query(query, context)\n",
        "        logger.info(f\"Parsed metrics query: CI Types={ci_types}, Metrics={metrics}, Time Range={time_range}\")\n",
        "\n",
        "        # Use RAG system to retrieve knowledge\n",
        "        if self.rag_system:\n",
        "            formatted_query = f\"I need to analyze metrics for CI types: {ci_types}, focusing on these metrics: {metrics}, over time range: {time_range}\"\n",
        "            rag_result = self.rag_system.query(\"observability\", formatted_query)\n",
        "            context.add_knowledge(\"observability\", rag_result)\n",
        "\n",
        "            # Create response\n",
        "            response = {\n",
        "                \"assessment\": rag_result[\"response\"],\n",
        "                \"sources\": rag_result[\"sources\"],\n",
        "                \"parameters\": {\n",
        "                    \"ci_types\": ci_types,\n",
        "                    \"metrics\": metrics,\n",
        "                    \"time_range\": time_range\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Update context - remember analyzed CI types for future reference\n",
        "            for ci_type in ci_types.split(\",\"):\n",
        "                ci_type = ci_type.strip()\n",
        "                if ci_type:\n",
        "                    context.update_state(f\"analyzed_{ci_type}\", True)\n",
        "\n",
        "            context.add_message(\"agent\", json.dumps(response))\n",
        "            context.set_last_agent(\"observability\")\n",
        "\n",
        "            return response\n",
        "        else:\n",
        "            return {\"error\": \"RAG system not available\"}\n",
        "\n",
        "    def _parse_metrics_query(self, query: str, context: AgentContext) -> tuple:\n",
        "        \"\"\"Parse CI types, metrics, and time range from query\"\"\"\n",
        "        # Default values\n",
        "        default_ci_types = \"router, switch\"\n",
        "        default_metrics = \"cpu_utilization, latency, memory_utilization\"\n",
        "        default_time_range = \"last_24h\"\n",
        "\n",
        "        ci_types = default_ci_types\n",
        "        metrics = default_metrics\n",
        "        time_range = default_time_range\n",
        "\n",
        "        # Try to extract from the query\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Extract CI types\n",
        "        if \"ci types:\" in query_lower or \"ci types :\" in query_lower:\n",
        "            parts = re.split(r\"ci types\\s*:\", query_lower, 1)\n",
        "            if len(parts) > 1:\n",
        "                ci_part = parts[1].split(\",\", 1)[0].strip()\n",
        "                if ci_part:\n",
        "                    ci_types = ci_part\n",
        "        elif \"devices:\" in query_lower or \"devices :\" in query_lower:\n",
        "            parts = re.split(r\"devices\\s*:\", query_lower, 1)\n",
        "            if len(parts) > 1:\n",
        "                ci_part = parts[1].split(\",\", 1)[0].strip()\n",
        "                if ci_part:\n",
        "                    ci_types = ci_part\n",
        "\n",
        "        # Extract metrics\n",
        "        if \"metrics:\" in query_lower or \"metrics :\" in query_lower:\n",
        "            parts = re.split(r\"metrics\\s*:\", query_lower, 1)\n",
        "            if len(parts) > 1:\n",
        "                metrics_part = parts[1].split(\",\", 1)[0].strip()\n",
        "                if metrics_part:\n",
        "                    metrics = metrics_part\n",
        "\n",
        "        # Extract time range\n",
        "        time_ranges = [\"last_1h\", \"last_6h\", \"last_12h\", \"last_24h\", \"last_3d\", \"last_7d\"]\n",
        "        for tr in time_ranges:\n",
        "            if tr in query_lower:\n",
        "                time_range = tr\n",
        "                break\n",
        "\n",
        "        # If devices were previously found in context, use them\n",
        "        if \"device\" in context.entity_memory and not \"ci types:\" in query_lower:\n",
        "            device_types = set()\n",
        "            for device_id, device in context.entity_memory[\"device\"].items():\n",
        "                if \"type\" in device:\n",
        "                    device_types.add(device[\"type\"].lower())\n",
        "            if device_types:\n",
        "                ci_types = \", \".join(device_types)\n",
        "\n",
        "        return ci_types, metrics, time_range\n",
        "\n",
        "class IncidentResolutionAgent(BaseAgent):\n",
        "    \"\"\"Agent specialized in incident management\"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: str):\n",
        "        capabilities = [\n",
        "            \"incident_analysis\",\n",
        "            \"resolution_guidance\",\n",
        "            \"impact_assessment\",\n",
        "            \"root_cause_analysis\"\n",
        "        ]\n",
        "        super().__init__(agent_id, \"incident_resolution\", capabilities)\n",
        "\n",
        "    def process(self, query: str, context: AgentContext) -> Dict[str, Any]:\n",
        "        \"\"\"Process an incident resolution query\"\"\"\n",
        "        logger.info(f\"IncidentResolutionAgent processing query: {query}\")\n",
        "\n",
        "        # Parse incident parameters\n",
        "        incident_id, title, description, status, priority, affected_cis = self._parse_incident_query(query)\n",
        "\n",
        "        # Use information from context to enhance query if needed\n",
        "        if affected_cis == \"\" and \"device\" in context.entity_memory:\n",
        "            # Use devices from memory if no CIs specified\n",
        "            affected_devices = []\n",
        "            for device_id, device in context.entity_memory[\"device\"].items():\n",
        "                affected_devices.append(device.get(\"name\", device_id))\n",
        "            if affected_devices:\n",
        "                affected_cis = \", \".join(affected_devices)\n",
        "\n",
        "        # Use RAG system to retrieve knowledge\n",
        "        if self.rag_system:\n",
        "            formatted_query = f\"Incident ID: {incident_id}\\nTitle: {title}\\nDescription: {description}\\nStatus: {status}\\nPriority: {priority}\\nAffected CIs: {affected_cis}\"\n",
        "            rag_result = self.rag_system.query(\"incident_resolution\", formatted_query)\n",
        "            context.add_knowledge(\"incident_resolution\", rag_result)\n",
        "\n",
        "            # Create response\n",
        "            response = {\n",
        "                \"summary\": rag_result[\"response\"],\n",
        "                \"sources\": rag_result[\"sources\"],\n",
        "                \"incident_details\": {\n",
        "                    \"id\": incident_id,\n",
        "                    \"title\": title,\n",
        "                    \"status\": status,\n",
        "                    \"priority\": priority,\n",
        "                    \"affected_cis\": affected_cis.split(\", \") if affected_cis else []\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Extract action items if any\n",
        "            action_items = self._extract_action_items(rag_result[\"response\"])\n",
        "            if action_items:\n",
        "                response[\"action_items\"] = action_items\n",
        "\n",
        "            # Update context\n",
        "            context.add_message(\"agent\", json.dumps(response))\n",
        "            context.set_last_agent(\"incident_resolution\")\n",
        "\n",
        "            # Remember this incident in context\n",
        "            context.remember_entity(\"incident\", incident_id, {\n",
        "                \"id\": incident_id,\n",
        "                \"title\": title,\n",
        "                \"status\": status,\n",
        "                \"priority\": priority\n",
        "            })\n",
        "\n",
        "            return response\n",
        "        else:\n",
        "            return {\"error\": \"RAG system not available\"}\n",
        "\n",
        "    def _parse_incident_query(self, query: str) -> tuple:\n",
        "        \"\"\"Parse incident details from query\"\"\"\n",
        "        # Default values\n",
        "        incident_id = \"INC-001\"\n",
        "        title = \"Network Issue\"\n",
        "        description = \"\"\n",
        "        status = \"open\"\n",
        "        priority = \"medium\"\n",
        "        affected_cis = \"\"\n",
        "\n",
        "        # Try to extract from the query\n",
        "        lines = query.split('\\n')\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if \":\" in line:\n",
        "                key, value = line.split(\":\", 1)\n",
        "                key = key.strip().lower()\n",
        "                value = value.strip()\n",
        "\n",
        "                if \"id\" in key:\n",
        "                    incident_id = value\n",
        "                elif \"title\" in key:\n",
        "                    title = value\n",
        "                elif \"description\" in key:\n",
        "                    description = value\n",
        "                elif \"status\" in key:\n",
        "                    status = value.lower()\n",
        "                elif \"priority\" in key:\n",
        "                    priority = value.lower()\n",
        "                elif \"affected\" in key and \"ci\" in key:\n",
        "                    affected_cis = value\n",
        "\n",
        "        return incident_id, title, description, status, priority, affected_cis\n",
        "\n",
        "    def _extract_action_items(self, text: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"Extract action items from incident resolution text\"\"\"\n",
        "        action_items = []\n",
        "\n",
        "        # Look for sections that indicate actions\n",
        "        sections = [\n",
        "            \"next steps\", \"action items\", \"recommendations\",\n",
        "            \"required actions\", \"follow-up\"\n",
        "        ]\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "        in_action_section = False\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line = line.strip().lower()\n",
        "\n",
        "            # Check if we're entering an action section\n",
        "            for section in sections:\n",
        "                if section in line and \":\" in line:\n",
        "                    in_action_section = True\n",
        "                    break\n",
        "\n",
        "            # If in action section, look for numbered or bulleted items\n",
        "            if in_action_section:\n",
        "                orig_line = lines[i].strip()\n",
        "                if (orig_line.startswith('- ') or orig_line.startswith('* ') or\n",
        "                    (len(orig_line) > 2 and orig_line[0].isdigit() and orig_line[1] == '.')):\n",
        "\n",
        "                    action = orig_line[2:].strip() if orig_line[1] in ['.', ' '] else orig_line[1:].strip()\n",
        "\n",
        "                    # Try to identify owner and deadline if present\n",
        "                    owner = \"Unassigned\"\n",
        "                    deadline = \"Not specified\"\n",
        "\n",
        "                    if \"owner:\" in action.lower() or \"assigned to:\" in action.lower():\n",
        "                        parts = re.split(r\"owner:|assigned to:\", action.lower(), 1)\n",
        "                        if len(parts) > 1:\n",
        "                            potential_owner = parts[1].split(\",\", 1)[0].strip()\n",
        "                            if potential_owner:\n",
        "                                owner = potential_owner\n",
        "                                action = parts[0].strip()\n",
        "\n",
        "                    if \"by:\" in action.lower() or \"deadline:\" in action.lower() or \"due:\" in action.lower():\n",
        "                        deadline_patterns = [r\"by:\", r\"deadline:\", r\"due:\"]\n",
        "                        for pattern in deadline_patterns:\n",
        "                            if re.search(pattern, action.lower()):\n",
        "                                parts = re.split(pattern, action.lower(), 1)\n",
        "                                if len(parts) > 1:\n",
        "                                    potential_deadline = parts[1].split(\",\", 1)[0].strip()\n",
        "                                    if potential_deadline:\n",
        "                                        deadline = potential_deadline\n",
        "                                        action = parts[0].strip()\n",
        "\n",
        "                    action_items.append({\n",
        "                        \"action\": action,\n",
        "                        \"owner\": owner,\n",
        "                        \"deadline\": deadline\n",
        "                    })\n",
        "\n",
        "        return action_items\n",
        "\n",
        "class MCPRegistry:\n",
        "    \"\"\"Central registry for MCP agents\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.agents = {}\n",
        "        self.rag_system = None\n",
        "        self.default_context = AgentContext()\n",
        "\n",
        "    def register_agent(self, agent: BaseAgent):\n",
        "        \"\"\"Register an agent in the system\"\"\"\n",
        "        self.agents[agent.agent_id] = agent\n",
        "        if self.rag_system:\n",
        "            agent.set_rag_system(self.rag_system)\n",
        "        logger.info(f\"Registered agent: {agent.agent_id} ({agent.agent_type})\")\n",
        "\n",
        "    def set_rag_system(self, rag_system):\n",
        "        \"\"\"Set the RAG system for all agents\"\"\"\n",
        "        self.rag_system = rag_system\n",
        "        for agent in self.agents.values():\n",
        "            agent.set_rag_system(rag_system)\n",
        "        logger.info(\"RAG system configured for all agents\")\n",
        "\n",
        "    def get_agent(self, agent_id: str) -> Optional[BaseAgent]:\n",
        "        \"\"\"Get an agent by ID\"\"\"\n",
        "        return self.agents.get(agent_id)\n",
        "\n",
        "    def get_agent_by_type(self, agent_type: str) -> Optional[BaseAgent]:\n",
        "        \"\"\"Get the first agent of a specific type\"\"\"\n",
        "        for agent in self.agents.values():\n",
        "            if agent.agent_type == agent_type:\n",
        "                return agent\n",
        "        return None\n",
        "\n",
        "    def find_agent_for_capability(self, capability: str) -> Optional[BaseAgent]:\n",
        "        \"\"\"Find an agent that can handle a specific capability\"\"\"\n",
        "        for agent in self.agents.values():\n",
        "            if agent.can_handle(capability):\n",
        "                return agent\n",
        "        return None\n",
        "\n",
        "    def find_agent_for_query(self, query: str) -> Optional[BaseAgent]:\n",
        "        \"\"\"Find the most appropriate agent for a query\"\"\"\n",
        "        # Simple keyword-based routing\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Define keywords for each agent type\n",
        "        routing_map = {\n",
        "            \"troubleshooting\": [\"problem\", \"issue\", \"error\", \"not working\", \"troubleshoot\", \"fix\", \"broken\"],\n",
        "            \"device_search\": [\"find\", \"search\", \"device\", \"router\", \"switch\", \"firewall\", \"topology\"],\n",
        "            \"knowledge_base\": [\"what is\", \"how to\", \"explain\", \"documentation\", \"best practice\"],\n",
        "            \"observability\": [\"monitor\", \"metrics\", \"performance\", \"trend\", \"utilization\", \"health\"],\n",
        "            \"incident_resolution\": [\"incident\", \"outage\", \"resolution\", \"root cause\", \"impact\"]\n",
        "        }\n",
        "\n",
        "        # Score each agent type based on keyword matches\n",
        "        scores = {agent_type: 0 for agent_type in routing_map}\n",
        "        for agent_type, keywords in routing_map.items():\n",
        "            for keyword in keywords:\n",
        "                if keyword in query_lower:\n",
        "                    scores[agent_type] += 1\n",
        "\n",
        "        # Check for specific formats that clearly indicate agent type\n",
        "        if \"incident id:\" in query_lower:\n",
        "            scores[\"incident_resolution\"] += 10\n",
        "        elif \"ci types:\" in query_lower or \"metrics:\" in query_lower:\n",
        "            scores[\"observability\"] += 10\n",
        "        elif \"document type:\" in query_lower:\n",
        "            scores[\"knowledge_base\"] += 10\n",
        "\n",
        "        # Get the agent type with the highest score\n",
        "        if any(scores.values()):\n",
        "            best_agent_type = max(scores, key=scores.get)\n",
        "            logger.info(f\"Query routed to {best_agent_type} agent (score: {scores[best_agent_type]})\")\n",
        "\n",
        "            # Find an agent of this type\n",
        "            for agent in self.agents.values():\n",
        "                if agent.agent_type == best_agent_type:\n",
        "                    return agent\n",
        "\n",
        "        # If no agent scores high enough to handle the query, the system falls back to using the Knowledge Base Agent\n",
        "        logger.info(\"No specific agent match, falling back to knowledge base agent\")\n",
        "        for agent in self.agents.values():\n",
        "            if agent.agent_type == \"knowledge_base\":\n",
        "                return agent\n",
        "\n",
        "        # Last resort: return the first agent /If for some reason the Knowledge Base Agent isn't available, the system will use the first agent in the registry\n",
        "        return next(iter(self.agents.values())) if self.agents else None\n",
        "\n",
        "    def process_query(self, query: str, agent_type: str = None, context: Optional[AgentContext] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Process a query with the most appropriate or specified agent\"\"\"\n",
        "        if context is None:\n",
        "            context = self.default_context\n",
        "\n",
        "        # Find the appropriate agent\n",
        "        agent = None\n",
        "        if agent_type:\n",
        "            # If agent type is specified, use it directly\n",
        "            agent = self.get_agent_by_type(agent_type)\n",
        "\n",
        "        if not agent:\n",
        "            # Otherwise find the most appropriate agent\n",
        "            agent = self.find_agent_for_query(query)\n",
        "\n",
        "        if not agent:\n",
        "            return {\"error\": \"No suitable agent found\"}\n",
        "\n",
        "        # Update context with the user query\n",
        "        context.add_message(\"user\", query)\n",
        "\n",
        "        # Process the query\n",
        "        result = agent.process(query, context)\n",
        "\n",
        "        # Enrich result with agent information\n",
        "        result[\"agent_id\"] = agent.agent_id\n",
        "        result[\"agent_type\"] = agent.agent_type\n",
        "\n",
        "        return result\n",
        "\n",
        "# Factory function to create a complete MCP system\n",
        "def create_mcp_system(rag_system, langgraph_agent=None):\n",
        "    \"\"\"Create and initialize a complete MCP system\"\"\"\n",
        "    # Create registry\n",
        "    registry = MCPRegistry()\n",
        "\n",
        "    # Create all agents\n",
        "    troubleshooting_agent = TroubleshootingAgent(\"Troubleshoot-IDP\")\n",
        "    device_search_agent = DeviceSearchAgent(\"Device-search-IDP\", langgraph_agent)\n",
        "    knowledge_agent = KnowledgeBaseAgent(\"KnowledgeBase-IDP\")\n",
        "    observability_agent = ObservabilityAgent(\"Observability-IDP\")\n",
        "    incident_agent = IncidentResolutionAgent(\"Incident-IDP\")\n",
        "\n",
        "    # Register agents\n",
        "    registry.register_agent(troubleshooting_agent)\n",
        "    registry.register_agent(device_search_agent)\n",
        "    registry.register_agent(knowledge_agent)\n",
        "    registry.register_agent(observability_agent)\n",
        "    registry.register_agent(incident_agent)\n",
        "\n",
        "    # Set RAG system\n",
        "    registry.set_rag_system(rag_system)\n",
        "\n",
        "    return registry\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYYdqsk56gAv"
      },
      "source": [
        "### LangGraph Enabled Device Search provide\n",
        "- Structured Workflow Management - Edge/Node definition\n",
        "- Query Parsing - Device Search - Topology Analysis for device relationships -Result Formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk3sW-4FLm9l",
        "outputId": "1e6bedc8-ad54-4b40-9802-96b7a2e1f1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing langgraph_device_search_agent.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile langgraph_device_search_agent.py\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "from typing import Dict, List, Any, Optional, Union\n",
        "from copy import deepcopy\n",
        "\n",
        "# Import required libraries\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"LangGraphDeviceSearchAgent\")\n",
        "\n",
        "# Define the device search state with improved type handling\n",
        "class DeviceSearchState(BaseModel):\n",
        "    \"\"\"State for the device search workflow\"\"\"\n",
        "    # Accept both dict and string types for query\n",
        "    query: Union[Dict[str, Any], str, None] = Field(default_factory=dict)\n",
        "    search_criteria: Dict[str, Any] = Field(default_factory=dict)\n",
        "    found_devices: List[Dict[str, Any]] = Field(default_factory=list)\n",
        "    upstream_devices: Dict[str, List[Dict[str, Any]]] = Field(default_factory=dict)\n",
        "    downstream_devices: Dict[str, List[Dict[str, Any]]] = Field(default_factory=dict)\n",
        "    affected_services: Dict[str, List[Dict[str, Any]]] = Field(default_factory=dict)\n",
        "    error: Optional[str] = None\n",
        "    status: str = \"initialized\"\n",
        "    rag_context: Optional[str] = None\n",
        "\n",
        "class LangGraphDeviceSearchAgent:\n",
        "    \"\"\"LangGraph-based Device Search Agent integrated with RAG\"\"\"\n",
        "\n",
        "    def __init__(self, llm, rag_system=None):\n",
        "        \"\"\"Initialize the agent with LLM and RAG system\"\"\"\n",
        "        self.llm = llm\n",
        "        self.rag_system = rag_system\n",
        "        self.graph = self._build_graph()\n",
        "        logger.info(\"LangGraph Device Search Agent initialized\")\n",
        "\n",
        "    def _build_graph(self):\n",
        "        \"\"\"Build the LangGraph workflow\"\"\"\n",
        "        # Create the graph with the state\n",
        "        workflow = StateGraph(DeviceSearchState)\n",
        "\n",
        "        # Add nodes for each step\n",
        "        workflow.add_node(\"parse_query\", self.parse_query)\n",
        "        workflow.add_node(\"search_devices\", self.search_devices)\n",
        "        workflow.add_node(\"analyze_topology\", self.analyze_topology)\n",
        "        workflow.add_node(\"format_results\", self.format_results)\n",
        "\n",
        "        # Add edges to define the flow\n",
        "        workflow.add_edge(\"parse_query\", \"search_devices\")\n",
        "        workflow.add_edge(\"search_devices\", \"analyze_topology\")\n",
        "        workflow.add_edge(\"analyze_topology\", \"format_results\")\n",
        "        workflow.add_edge(\"format_results\", END)\n",
        "\n",
        "        # Add conditional edges for error handling\n",
        "        workflow.add_conditional_edges(\n",
        "            \"parse_query\",\n",
        "            lambda state: \"search_devices\" if state.error is None else END\n",
        "        )\n",
        "\n",
        "        workflow.add_conditional_edges(\n",
        "            \"search_devices\",\n",
        "            lambda state: \"analyze_topology\" if state.error is None else END\n",
        "        )\n",
        "\n",
        "        # Set the entry point\n",
        "        workflow.set_entry_point(\"parse_query\")\n",
        "\n",
        "        # Compile the graph\n",
        "        return workflow.compile()\n",
        "\n",
        "    def parse_query(self, state: DeviceSearchState) -> DeviceSearchState:\n",
        "        \"\"\"Parse query to extract search criteria with improved string handling\"\"\"\n",
        "        logger.info(\"Parsing query\")\n",
        "        new_state = deepcopy(state)\n",
        "\n",
        "        try:\n",
        "            # Handle different query types\n",
        "            if isinstance(state.query, str):\n",
        "                logger.info(f\"Converting string query to dict: {state.query}\")\n",
        "                new_state.search_criteria = {\"description\": state.query}\n",
        "            elif isinstance(state.query, dict):\n",
        "                # If already a dict, use it directly\n",
        "                new_state.search_criteria = state.query\n",
        "            elif state.query is None:\n",
        "                # Handle None by creating an empty dict\n",
        "                new_state.search_criteria = {}\n",
        "            else:\n",
        "                # Handle any other type\n",
        "                new_state.search_criteria = {\"raw_value\": str(state.query)}\n",
        "\n",
        "            return new_state\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in parse_query: {e}\")\n",
        "            new_state.error = f\"Error parsing query: {str(e)}\"\n",
        "            return new_state\n",
        "\n",
        "    def search_devices(self, state: DeviceSearchState) -> DeviceSearchState:\n",
        "        \"\"\"Search for devices matching criteria with case-insensitive field handling\"\"\"\n",
        "        logger.info(\"Searching devices\")\n",
        "        new_state = deepcopy(state)\n",
        "\n",
        "        try:\n",
        "            # Extract query text for RAG and tracking\n",
        "            query_text = \"\"\n",
        "            if isinstance(state.query, str):\n",
        "                query_text = state.query\n",
        "            elif isinstance(state.query, dict) and \"description\" in state.query:\n",
        "                query_text = state.query[\"description\"]\n",
        "            else:\n",
        "                query_text = json.dumps(state.search_criteria)\n",
        "\n",
        "            # Get RAG context if available\n",
        "            context = \"Use your knowledge of network topologies.\"\n",
        "            if self.rag_system:\n",
        "                try:\n",
        "                    # Query the RAG system\n",
        "                    rag_result = self.rag_system.query(\"device_search\", query_text)\n",
        "                    if rag_result and \"response\" in rag_result:\n",
        "                        # Make the RAG response safe for string formatting\n",
        "                        rag_response = rag_result['response']\n",
        "                        safe_response = rag_response.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "\n",
        "                        context = f\"Use this information from our knowledge base:\\n{safe_response}\"\n",
        "                        new_state.rag_context = context\n",
        "\n",
        "                        logger.info(f\"Retrieved {len(rag_result.get('sources', []))} knowledge base entries\")\n",
        "                except Exception as rag_error:\n",
        "                    logger.warning(f\"Error using RAG system: {rag_error}\")\n",
        "\n",
        "            # Use RAG and LLM to simulate device search\n",
        "            prompt = ChatPromptTemplate.from_template(\"\"\"<|im_start|>system\n",
        "You are a network topology expert who can search and find devices in a network.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "Search for network devices matching these criteria:\n",
        "{criteria}\n",
        "\n",
        "{context}\n",
        "\n",
        "Generate a list of found devices with these EXACT properties (use exactly these field names, all lowercase):\n",
        "- ci_id: Device identifier (use format like \"R001\" for routers, \"S001\" for switches, \"FW001\" for firewalls)\n",
        "- name: Descriptive name\n",
        "- ci_type: Device type (router, switch, firewall, etc.) - MUST BE ALL LOWERCASE\n",
        "- status: Current status (active, inactive, warning, etc.)\n",
        "- location: Physical location\n",
        "- importance: Importance in the network (use values like \"high\", \"medium\", \"low\", \"critical\")\n",
        "\n",
        "Respond ONLY with a JSON list of devices that match the criteria. Use EXACTLY the field names shown above.\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\")\n",
        "\n",
        "            # Format the criteria for the prompt\n",
        "            criteria_str = json.dumps(state.search_criteria, indent=2)\n",
        "\n",
        "            # Handle format string issues\n",
        "            safe_criteria_str = criteria_str.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "\n",
        "            # Generate device list\n",
        "            try:\n",
        "                formatted_prompt = prompt.format(criteria=safe_criteria_str, context=context)\n",
        "                response = self.llm.invoke(formatted_prompt)\n",
        "            except ValueError as ve:\n",
        "                logger.warning(f\"Format string error: {ve}, using alternative formatting\")\n",
        "                template = prompt.template\n",
        "                safe_template = template.replace(\"{criteria}\", criteria_str).replace(\"{context}\",\n",
        "                                          \"Use your knowledge of network devices to find matching devices.\")\n",
        "                response = self.llm.invoke(safe_template)\n",
        "\n",
        "            # Use JSON extraction and parsing\n",
        "            devices = self._extract_and_fix_json(response, query_text)\n",
        "\n",
        "            # Log received devices for debugging\n",
        "            logger.info(f\"Devices before standardization: {devices}\")\n",
        "\n",
        "            # Case-insensitive field standardization\n",
        "            validated_devices = []\n",
        "            for device in devices:\n",
        "                # Create a lowercase keys dictionary for case-insensitive lookup\n",
        "                lower_device = {k.lower(): v for k, v in device.items()}\n",
        "\n",
        "                # Now extract fields with fallbacks using the lowercase dictionary\n",
        "                standardized_device = {\n",
        "                    \"ci_id\": None,\n",
        "                    \"name\": None,\n",
        "                    \"ci_type\": None,\n",
        "                    \"status\": None,\n",
        "                    \"location\": None,\n",
        "                    \"importance\": None\n",
        "                }\n",
        "\n",
        "                # ci_id field\n",
        "                if \"ci_id\" in lower_device:\n",
        "                    standardized_device[\"ci_id\"] = lower_device[\"ci_id\"]\n",
        "                elif \"id\" in lower_device:\n",
        "                    standardized_device[\"ci_id\"] = lower_device[\"id\"]\n",
        "                else:\n",
        "                    # Determine ID based on type if possible\n",
        "                    if \"firewall\" in query_text.lower():\n",
        "                        standardized_device[\"ci_id\"] = \"FW001\"\n",
        "                    elif \"router\" in query_text.lower():\n",
        "                        standardized_device[\"ci_id\"] = f\"R{len(validated_devices)+1:03d}\"\n",
        "                    elif \"switch\" in query_text.lower():\n",
        "                        standardized_device[\"ci_id\"] = f\"S{len(validated_devices)+1:03d}\"\n",
        "                    else:\n",
        "                        standardized_device[\"ci_id\"] = f\"DEV{len(validated_devices)+1:03d}\"\n",
        "\n",
        "                # name field\n",
        "                if \"name\" in lower_device:\n",
        "                    standardized_device[\"name\"] = lower_device[\"name\"]\n",
        "                elif \"description\" in lower_device:\n",
        "                    standardized_device[\"name\"] = lower_device[\"description\"]\n",
        "                else:\n",
        "                    device_type = \"Device\"\n",
        "                    if \"ci_type\" in lower_device:\n",
        "                        device_type = lower_device[\"ci_type\"]\n",
        "                    elif \"type\" in lower_device:\n",
        "                        device_type = lower_device[\"type\"]\n",
        "                    standardized_device[\"name\"] = f\"{device_type} {len(validated_devices)+1}\"\n",
        "\n",
        "                # ci_type field - Standardization for Gradio display\n",
        "                if \"ci_type\" in lower_device:\n",
        "                    standardized_device[\"ci_type\"] = str(lower_device[\"ci_type\"]).lower()\n",
        "                elif \"type\" in lower_device:\n",
        "                    standardized_device[\"ci_type\"] = str(lower_device[\"type\"]).lower()\n",
        "                else:\n",
        "                    # Determine type from ID if possible\n",
        "                    if standardized_device[\"ci_id\"].startswith(\"R\"):\n",
        "                        standardized_device[\"ci_type\"] = \"router\"\n",
        "                    elif standardized_device[\"ci_id\"].startswith(\"S\"):\n",
        "                        standardized_device[\"ci_type\"] = \"switch\"\n",
        "                    elif standardized_device[\"ci_id\"].startswith(\"FW\"):\n",
        "                        standardized_device[\"ci_type\"] = \"firewall\"\n",
        "                    else:\n",
        "                        standardized_device[\"ci_type\"] = \"unknown\"\n",
        "\n",
        "                # status field\n",
        "                if \"status\" in lower_device:\n",
        "                    standardized_device[\"status\"] = str(lower_device[\"status\"])\n",
        "                elif \"state\" in lower_device:\n",
        "                    standardized_device[\"status\"] = str(lower_device[\"state\"])\n",
        "                else:\n",
        "                    standardized_device[\"status\"] = \"active\"\n",
        "\n",
        "                # location field\n",
        "                if \"location\" in lower_device:\n",
        "                    standardized_device[\"location\"] = str(lower_device[\"location\"])\n",
        "                elif \"site\" in lower_device:\n",
        "                    standardized_device[\"location\"] = str(lower_device[\"site\"])\n",
        "                else:\n",
        "                    standardized_device[\"location\"] = \"Unknown\"\n",
        "\n",
        "                # importance field\n",
        "                if \"importance\" in lower_device:\n",
        "                    standardized_device[\"importance\"] = str(lower_device[\"importance\"]).lower()\n",
        "                elif \"criticality\" in lower_device:\n",
        "                    standardized_device[\"importance\"] = str(lower_device[\"criticality\"]).lower()\n",
        "                elif \"priority\" in lower_device:\n",
        "                    standardized_device[\"importance\"] = str(lower_device[\"priority\"]).lower()\n",
        "                else:\n",
        "                    standardized_device[\"importance\"] = \"medium\"\n",
        "\n",
        "                # Make sure all values are strings to avoid error\n",
        "                for key, value in standardized_device.items():\n",
        "                    if value is None:\n",
        "                        if key == \"ci_id\":\n",
        "                            standardized_device[key] = f\"DEV{len(validated_devices)+1:03d}\"\n",
        "                        elif key == \"name\":\n",
        "                            standardized_device[key] = \"Unknown Device\"\n",
        "                        elif key == \"ci_type\":\n",
        "                            standardized_device[key] = \"unknown\"\n",
        "                        elif key == \"status\":\n",
        "                            standardized_device[key] = \"active\"\n",
        "                        elif key == \"location\":\n",
        "                            standardized_device[key] = \"Unknown\"\n",
        "                        elif key == \"importance\":\n",
        "                            standardized_device[key] = \"medium\"\n",
        "                    elif not isinstance(value, str):\n",
        "                        standardized_device[key] = str(value)\n",
        "\n",
        "                logger.info(f\"Standardized device: {standardized_device}\")\n",
        "                validated_devices.append(standardized_device)\n",
        "\n",
        "            new_state.found_devices = validated_devices\n",
        "            logger.info(f\"Successfully processed {len(validated_devices)} standardized devices\")\n",
        "\n",
        "            return new_state\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in search_devices: {e}\")\n",
        "            new_state.error = f\"Error searching devices: {str(e)}\"\n",
        "            return new_state\n",
        "\n",
        "    def _extract_and_fix_json(self, response, query_text=\"\"):\n",
        "        \"\"\"Extract and fix JSON from LLM response with robust error handling\"\"\"\n",
        "        import re\n",
        "        import json\n",
        "\n",
        "        # Try multiple approaches to extract valid JSON\n",
        "        json_str = None\n",
        "\n",
        "        # First try: Look for JSON code block\n",
        "        json_match = re.search(r'```(?:json)?\\s*\\n([\\s\\S]*?)\\n```', response, re.DOTALL)\n",
        "        if json_match:\n",
        "            json_str = json_match.group(1).strip()\n",
        "        else:\n",
        "            # Second try: Look for array pattern\n",
        "            json_match = re.search(r'(\\[\\s*\\{.*\\}\\s*\\])', response, re.DOTALL)\n",
        "            if json_match:\n",
        "                json_str = json_match.group(1).strip()\n",
        "            else:\n",
        "                # Third try: Scan for first [ to last ]\n",
        "                start = response.find('[')\n",
        "                end = response.rfind(']')\n",
        "                if start != -1 and end != -1 and end > start:\n",
        "                    json_str = response[start:end+1].strip()\n",
        "                else:\n",
        "                    # Last resort: Check if the entire response might be JSON with some prefix\n",
        "                    clean_response = response.strip()\n",
        "                    start = clean_response.find('[')\n",
        "                    if start != -1:\n",
        "                        json_str = clean_response[start:].strip()\n",
        "\n",
        "        # If JSON extraction was not possible, use fallback\n",
        "        if not json_str:\n",
        "            logger.warning(\"Couldn't extract JSON from response, using fallback\")\n",
        "            return self._get_fallback_devices(query_text)\n",
        "\n",
        "        logger.info(f\"Extracted JSON string: {json_str[:100]}...\")\n",
        "\n",
        "        # Cleanup the JSON string before parsing\n",
        "        try:\n",
        "            # Remove control characters\n",
        "            json_str = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', json_str)\n",
        "\n",
        "            # Fix common JSON formatting issues\n",
        "            json_str = re.sub(r',\\s*\\}', '}', json_str)  # Fix trailing commas in objects\n",
        "            json_str = re.sub(r',\\s*\\]', ']', json_str)  # Fix trailing commas in arrays\n",
        "            json_str = re.sub(r'([{,]\\s*)(\\w+)(\\s*:)', r'\\1\"\\2\"\\3', json_str)  # Quote property names\n",
        "\n",
        "            # Try to parse the JSON with various fixes\n",
        "            try:\n",
        "                parsed_data = json.loads(json_str)\n",
        "\n",
        "                # Check for field/value format (common in some LLM outputs)\n",
        "                if isinstance(parsed_data, list) and len(parsed_data) > 0 and \"field\" in parsed_data[0] and \"value\" in parsed_data[0]:\n",
        "                    logger.info(\"Detected field/value format, converting to standard format\")\n",
        "\n",
        "                    # Convert field/value format to standard device object\n",
        "                    device_fields = {}\n",
        "                    for item in parsed_data:\n",
        "                        field = item.get(\"field\", \"\").lower()\n",
        "                        value = item.get(\"value\", \"\")\n",
        "\n",
        "                        # Map fields to standard names\n",
        "                        if field == \"description\" or field == \"name\":\n",
        "                            device_fields[\"name\"] = value\n",
        "                        elif field == \"ci_id\" or field == \"id\" or field == \"device id\":\n",
        "                            device_fields[\"ci_id\"] = value\n",
        "                        elif field == \"ci_type\" or field == \"type\" or field == \"device type\":\n",
        "                            device_fields[\"ci_type\"] = value\n",
        "                        elif field == \"status\":\n",
        "                            device_fields[\"status\"] = value\n",
        "                        elif field == \"location\":\n",
        "                            device_fields[\"location\"] = value\n",
        "                        elif field == \"importance\":\n",
        "                            device_fields[\"importance\"] = value\n",
        "\n",
        "                    # Create a proper device object\n",
        "                    if device_fields:\n",
        "                        # Create a device using the extracted fields\n",
        "                        # Use \"FW001\" as default ci_id if query contains \"firewall\"\n",
        "                        default_ci_id = \"FW001\" if \"firewall\" in query_text.lower() else \"DEV001\"\n",
        "\n",
        "                        device = {\n",
        "                            \"ci_id\": device_fields.get(\"ci_id\", default_ci_id),\n",
        "                            \"name\": device_fields.get(\"name\", \"Unknown Device\"),\n",
        "                            \"ci_type\": device_fields.get(\"ci_type\", \"unknown\"),\n",
        "                            \"status\": device_fields.get(\"status\", \"active\"),\n",
        "                            \"location\": device_fields.get(\"location\", \"Unknown\"),\n",
        "                            \"importance\": device_fields.get(\"importance\", \"medium\")\n",
        "                        }\n",
        "                        return [device]\n",
        "                    else:\n",
        "                        # If we couldn't extract fields, use fallback\n",
        "                        return self._get_fallback_devices(query_text)\n",
        "\n",
        "                # Handle other list formats that may not be device objects\n",
        "                if isinstance(parsed_data, list) and len(parsed_data) > 0:\n",
        "                    # Check if these are proper device objects\n",
        "                    if all(\"name\" in item or \"ci_id\" in item for item in parsed_data):\n",
        "                        return parsed_data\n",
        "                    else:\n",
        "                        # Not device objects, use fallback\n",
        "                        logger.warning(\"Parsed JSON doesn't contain proper device objects\")\n",
        "                        return self._get_fallback_devices(query_text)\n",
        "\n",
        "                # If not a list, try to make it one\n",
        "                if not isinstance(parsed_data, list):\n",
        "                    if isinstance(parsed_data, dict):\n",
        "                        return [parsed_data]\n",
        "                    else:\n",
        "                        return self._get_fallback_devices(query_text)\n",
        "\n",
        "                return parsed_data\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                logger.warning(f\"Initial JSON parse failed: {e}, attempting fixes\")\n",
        "\n",
        "                # Try with single quote replacement\n",
        "                try:\n",
        "                    json_str = json_str.replace(\"'\", '\"')\n",
        "                    parsed_data = json.loads(json_str)\n",
        "                    return parsed_data\n",
        "                except json.JSONDecodeError as e2:\n",
        "                    logger.warning(f\"Single quote fix failed: {e2}, attempting line-by-line parsing\")\n",
        "\n",
        "                    # Try to parse the JSON line by line\n",
        "                    try:\n",
        "                        device_matches = re.finditer(r'\\{[^{}]*\\}', json_str)\n",
        "                        devices = []\n",
        "\n",
        "                        for match in device_matches:\n",
        "                            device_str = match.group(0)\n",
        "                            try:\n",
        "                                device_str = device_str.replace(\"'\", '\"')\n",
        "                                device_str = re.sub(r'([{,]\\s*)(\\w+)(\\s*:)', r'\\1\"\\2\"\\3', device_str)\n",
        "                                device_obj = json.loads(device_str)\n",
        "                                devices.append(device_obj)\n",
        "                            except json.JSONDecodeError:\n",
        "                                logger.warning(f\"Skipping invalid device object: {device_str[:50]}...\")\n",
        "\n",
        "                        if devices:\n",
        "                            logger.info(f\"Successfully parsed {len(devices)} devices from line-by-line approach\")\n",
        "                            return devices\n",
        "                        else:\n",
        "                            logger.warning(\"Line-by-line parsing yielded no valid devices\")\n",
        "                    except Exception as e3:\n",
        "                        logger.warning(f\"Line-by-line parsing failed: {e3}\")\n",
        "\n",
        "                    # Manual extraction with regex - as last resort solution\n",
        "                    try:\n",
        "                        ci_ids = re.findall(r'[\"\\']ci_id[\"\\']\\s*:\\s*[\"\\']([^\"\\']+)[\"\\']', json_str)\n",
        "                        names = re.findall(r'[\"\\']name[\"\\']\\s*:\\s*[\"\\']([^\"\\']+)[\"\\']', json_str)\n",
        "                        types = re.findall(r'[\"\\'](?:ci_type|type)[\"\\']\\s*:\\s*[\"\\']([^\"\\']+)[\"\\']', json_str)\n",
        "\n",
        "                        if ci_ids or names:\n",
        "                            devices = []\n",
        "                            # Use the longest list as our base\n",
        "                            max_length = max(len(ci_ids), len(names), len(types))\n",
        "\n",
        "                            for i in range(max_length):\n",
        "                                device = {\n",
        "                                    \"ci_id\": ci_ids[i] if i < len(ci_ids) else f\"DEV{i+1:03d}\",\n",
        "                                    \"name\": names[i] if i < len(names) else f\"Device {i+1}\",\n",
        "                                    \"ci_type\": types[i] if i < len(types) else \"unknown\",\n",
        "                                    \"status\": \"active\",\n",
        "                                    \"importance\": \"medium\",\n",
        "                                    \"location\": \"Unknown\"\n",
        "                                }\n",
        "                                devices.append(device)\n",
        "\n",
        "                            logger.info(f\"Manually constructed {len(devices)} devices from regex extraction\")\n",
        "                            return devices\n",
        "                    except Exception as e4:\n",
        "                        logger.warning(f\"Manual extraction failed: {e4}\")\n",
        "\n",
        "            # If all parsing attempts failed, use fallback\n",
        "            logger.warning(\"All JSON parsing attempts failed, using fallback devices\")\n",
        "            return self._get_fallback_devices(query_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error cleaning up JSON: {e}\")\n",
        "            return self._get_fallback_devices(query_text)\n",
        "\n",
        "    def _get_fallback_devices(self, query_text):\n",
        "        \"\"\"Generate fallback devices based on query text\"\"\"\n",
        "        devices = []\n",
        "\n",
        "        # Check for common device types in the query\n",
        "        if \"router\" in query_text.lower():\n",
        "            devices.append({\n",
        "                \"ci_id\": \"R001\",\n",
        "                \"name\": \"Core Router 1\",\n",
        "                \"ci_type\": \"router\",\n",
        "                \"status\": \"active\",\n",
        "                \"location\": \"Data Center\",\n",
        "                \"importance\": \"critical\"\n",
        "            })\n",
        "\n",
        "            if \"all\" in query_text.lower():\n",
        "                devices.append({\n",
        "                    \"ci_id\": \"R002\",\n",
        "                    \"name\": \"Core Router 2\",\n",
        "                    \"ci_type\": \"router\",\n",
        "                    \"status\": \"active\",\n",
        "                    \"location\": \"Backup Data Center\",\n",
        "                    \"importance\": \"critical\"\n",
        "                })\n",
        "\n",
        "        elif \"switch\" in query_text.lower():\n",
        "            devices.append({\n",
        "                \"ci_id\": \"S001\",\n",
        "                \"name\": \"Distribution Switch 1\",\n",
        "                \"ci_type\": \"switch\",\n",
        "                \"status\": \"active\",\n",
        "                \"location\": \"Main Office\",\n",
        "                \"importance\": \"high\"\n",
        "            })\n",
        "\n",
        "            if \"all\" in query_text.lower():\n",
        "                devices.append({\n",
        "                    \"ci_id\": \"S002\",\n",
        "                    \"name\": \"Access Switch 1\",\n",
        "                    \"ci_type\": \"switch\",\n",
        "                    \"status\": \"active\",\n",
        "                    \"location\": \"Branch Office\",\n",
        "                    \"importance\": \"medium\"\n",
        "                })\n",
        "\n",
        "        elif \"firewall\" in query_text.lower():\n",
        "            devices.append({\n",
        "                \"ci_id\": \"FW001\",\n",
        "                \"name\": \"Edge Firewall\",\n",
        "                \"ci_type\": \"firewall\",\n",
        "                \"status\": \"active\",\n",
        "                \"location\": \"Data Center\",\n",
        "                \"importance\": \"critical\"\n",
        "            })\n",
        "\n",
        "        # If nothing matched or we need more devices, add a generic one\n",
        "        if not devices or \"all\" in query_text.lower():\n",
        "            devices.append({\n",
        "                \"ci_id\": \"DEV001\",\n",
        "                \"name\": \"Generic Network Device\",\n",
        "                \"ci_type\": \"unknown\",\n",
        "                \"status\": \"active\",\n",
        "                \"location\": \"Network Core\",\n",
        "                \"importance\": \"medium\"\n",
        "            })\n",
        "\n",
        "        return devices\n",
        "\n",
        "    def analyze_topology(self, state: DeviceSearchState) -> DeviceSearchState:\n",
        "        \"\"\"Analyze upstream and downstream devices\"\"\"\n",
        "        logger.info(\"Analyzing topology\")\n",
        "        new_state = deepcopy(state)\n",
        "\n",
        "        try:\n",
        "            # Process each found device\n",
        "            for device in new_state.found_devices:\n",
        "                device_id = device.get(\"ci_id\")\n",
        "\n",
        "                # Use RAG and LLM to simulate topology analysis\n",
        "                if device_id:\n",
        "                    # Generate upstream devices\n",
        "                    upstream = self._generate_connected_devices(device, \"upstream\")\n",
        "                    new_state.upstream_devices[device_id] = upstream\n",
        "\n",
        "                    # Generate downstream devices\n",
        "                    downstream = self._generate_connected_devices(device, \"downstream\")\n",
        "                    new_state.downstream_devices[device_id] = downstream\n",
        "\n",
        "                    # Generate affected services\n",
        "                    services = self._generate_affected_services(device)\n",
        "                    new_state.affected_services[device_id] = services\n",
        "\n",
        "            return new_state\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in analyze_topology: {e}\")\n",
        "            import traceback\n",
        "            logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
        "            new_state.error = f\"Error analyzing topology: {str(e)}\"\n",
        "            return new_state\n",
        "\n",
        "    def _generate_connected_devices(self, device, direction):\n",
        "        \"\"\"Generate connected devices with consistent field names\"\"\"\n",
        "        # Get standardized device information\n",
        "        device_type = device.get(\"ci_type\", \"\").lower()\n",
        "        device_id = device.get(\"ci_id\", \"\")\n",
        "        device_name = device.get(\"name\", \"unknown-device\")\n",
        "\n",
        "        result = []\n",
        "        try:\n",
        "            # Generate different connections based on device type\n",
        "            if device_type == \"router\":\n",
        "                if direction == \"upstream\":\n",
        "                    # Router upstream connections\n",
        "                    numeric_part = 1\n",
        "                    if device_id.startswith('R') and device_id[1:].isdigit():\n",
        "                        numeric_part = int(device_id[1:])\n",
        "\n",
        "                    upstream_id = f\"R{numeric_part - 1:03d}\" if numeric_part > 1 else \"WAN001\"\n",
        "                    upstream_name = f\"core-router-{numeric_part - 1:02d}\" if numeric_part > 1 else \"wan-edge-01\"\n",
        "\n",
        "                    result = [{\n",
        "                        \"ci_id\": upstream_id,\n",
        "                        \"name\": upstream_name,\n",
        "                        \"ci_type\": \"router\",\n",
        "                        \"status\": \"active\",\n",
        "                        \"location\": \"Network Core\",\n",
        "                        \"importance\": \"critical\"\n",
        "                    }]\n",
        "                else:\n",
        "                    # Router downstream connections\n",
        "                    switch_suffix = device_id.replace(\"R\", \"\").replace(\"-\", \"\")[:3]\n",
        "                    if not switch_suffix.isalnum():\n",
        "                        switch_suffix = \"001\"\n",
        "\n",
        "                    result = [\n",
        "                        {\n",
        "                            \"ci_id\": f\"S{switch_suffix}A\",\n",
        "                            \"name\": f\"distribution-switch-{device_name[:3]}a\",\n",
        "                            \"ci_type\": \"switch\",\n",
        "                            \"status\": \"active\",\n",
        "                            \"location\": device.get(\"location\", \"Unknown\"),\n",
        "                            \"importance\": \"high\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"ci_id\": f\"S{switch_suffix}B\",\n",
        "                            \"name\": f\"distribution-switch-{device_name[:3]}b\",\n",
        "                            \"ci_type\": \"switch\",\n",
        "                            \"status\": \"active\",\n",
        "                            \"location\": device.get(\"location\", \"Unknown\"),\n",
        "                            \"importance\": \"high\"\n",
        "                        }\n",
        "                    ]\n",
        "            elif device_type == \"switch\":\n",
        "                if direction == \"upstream\":\n",
        "                    # Switches connect upstream to routers or other switches\n",
        "                    if \"access\" in str(device_name).lower():\n",
        "                        # Create a descriptive ID\n",
        "                        dist_id = device_id.replace(\"S\", \"\").split(\"1\")[0]\n",
        "                        if not dist_id:\n",
        "                            dist_id = \"01\"\n",
        "\n",
        "                        result = [\n",
        "                            {\n",
        "                                \"ci_id\": f\"S{dist_id}0\",\n",
        "                                \"name\": f\"distribution-switch-{dist_id}\",\n",
        "                                \"ci_type\": \"switch\",\n",
        "                                \"status\": \"active\",\n",
        "                                \"location\": device.get(\"location\", \"Unknown\"),\n",
        "                                \"importance\": \"high\"\n",
        "                            }\n",
        "                        ]\n",
        "                    else:\n",
        "                        # Create a sensible router ID\n",
        "                        router_suffix = device_id.replace(\"S\", \"\").replace(\"-\", \"\")[:2]\n",
        "                        if not router_suffix.isalnum():\n",
        "                            router_suffix = \"01\"  # Fallback suffix\n",
        "\n",
        "                        result = [\n",
        "                            {\n",
        "                                \"ci_id\": f\"R{router_suffix}\",\n",
        "                                \"name\": f\"core-router-{router_suffix}\",\n",
        "                                \"ci_type\": \"router\",\n",
        "                                \"status\": \"active\",\n",
        "                                \"location\": device.get(\"location\", \"Unknown\"),\n",
        "                                \"importance\": \"critical\"\n",
        "                            }\n",
        "                        ]\n",
        "                else:\n",
        "                    # Downstream devices\n",
        "                    device_suffix = device_id.replace(\"S\", \"\").replace(\"-\", \"\")[:3]\n",
        "                    if not device_suffix.isalnum():\n",
        "                        device_suffix = \"001\"  # Fallback\n",
        "\n",
        "                    if \"access\" in str(device_name).lower():\n",
        "                        result = [\n",
        "                            {\n",
        "                                \"ci_id\": f\"SRV{device_suffix}A\",\n",
        "                                \"name\": f\"server-{device_suffix}-rack-a\",\n",
        "                                \"ci_type\": \"server\",\n",
        "                                \"status\": \"active\",\n",
        "                                \"location\": device.get(\"location\", \"Unknown\"),\n",
        "                                \"importance\": \"high\"\n",
        "                            },\n",
        "                            {\n",
        "                                \"ci_id\": f\"SRV{device_suffix}B\",\n",
        "                                \"name\": f\"server-{device_suffix}-rack-b\",\n",
        "                                \"ci_type\": \"server\",\n",
        "                                \"status\": \"active\",\n",
        "                                \"location\": device.get(\"location\", \"Unknown\"),\n",
        "                                \"importance\": \"high\"\n",
        "                            }\n",
        "                        ]\n",
        "                    else:\n",
        "                        # Distribution switches connect to access switches\n",
        "                        result = [\n",
        "                            {\n",
        "                                \"ci_id\": f\"S{device_suffix}1\",\n",
        "                                \"name\": f\"access-switch-{device_suffix}-1\",\n",
        "                                \"ci_type\": \"switch\",\n",
        "                                \"status\": \"active\",\n",
        "                                \"location\": device.get(\"location\", \"Unknown\"),\n",
        "                                \"importance\": \"medium\"\n",
        "                            },\n",
        "                            {\n",
        "                                \"ci_id\": f\"S{device_suffix}2\",\n",
        "                                \"name\": f\"access-switch-{device_suffix}-2\",\n",
        "                                \"ci_type\": \"switch\",\n",
        "                                \"status\": \"active\",\n",
        "                                \"location\": device.get(\"location\", \"Unknown\"),\n",
        "                                \"importance\": \"medium\"\n",
        "                            }\n",
        "                        ]\n",
        "            elif device_type == \"firewall\":\n",
        "                if direction == \"upstream\":\n",
        "                    # Firewall connects upstream to router\n",
        "                    result = [\n",
        "                        {\n",
        "                            \"ci_id\": \"R001\",\n",
        "                            \"name\": \"core-router-01\",\n",
        "                            \"ci_type\": \"router\",\n",
        "                            \"status\": \"active\",\n",
        "                            \"location\": \"Network Core\",\n",
        "                            \"importance\": \"critical\"\n",
        "                        }\n",
        "                    ]\n",
        "                else:\n",
        "                  # Firewall connects downstream to DMZ and internal networks\n",
        "                    result = [\n",
        "                        {\n",
        "                            \"ci_id\": \"DMZ001\",\n",
        "                            \"name\": \"dmz-switch-01\",\n",
        "                            \"ci_type\": \"switch\",\n",
        "                            \"status\": \"active\",\n",
        "                            \"location\": \"DMZ\",\n",
        "                            \"importance\": \"high\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"ci_id\": \"S001\",\n",
        "                            \"name\": \"internal-switch-01\",\n",
        "                            \"ci_type\": \"switch\",\n",
        "                            \"status\": \"active\",\n",
        "                            \"location\": \"Internal Network\",\n",
        "                            \"importance\": \"high\"\n",
        "                        }\n",
        "                    ]\n",
        "\n",
        "            # If could not generate based on type, provide a  fallback\n",
        "            if not result:\n",
        "                hash_val = abs(hash(str(device_id))) % 1000  # Ensure positive hash value\n",
        "                result = [\n",
        "                    {\n",
        "                        \"ci_id\": f\"DEV{hash_val:03d}\",\n",
        "                        \"name\": f\"connected-device-{direction}\",\n",
        "                        \"ci_type\": \"generic\",\n",
        "                        \"status\": \"active\",\n",
        "                        \"location\": \"Unknown\",\n",
        "                        \"importance\": \"medium\"\n",
        "                    }\n",
        "                ]\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log the error but don't break the workflow\n",
        "            logger.warning(f\"Error generating connected devices for {device_id}: {e}\")\n",
        "            # Return a generic device as fallback\n",
        "            hash_val = abs(hash(str(device_id))) % 1000  # Ensure positive hash value\n",
        "            result = [\n",
        "                {\n",
        "                    \"ci_id\": f\"DEV{hash_val:03d}\",\n",
        "                    \"name\": f\"connected-device-{direction}\",\n",
        "                    \"ci_type\": \"generic\",\n",
        "                    \"status\": \"active\",\n",
        "                    \"location\": \"Unknown\",\n",
        "                    \"importance\": \"medium\"\n",
        "                }\n",
        "            ]\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _generate_affected_services(self, device):\n",
        "        \"\"\"Generate affected services for a device with improved type handling\"\"\"\n",
        "        device_type = device.get(\"ci_type\", \"\")\n",
        "\n",
        "        # Fix for the error: Check type before calling .lower()\n",
        "        importance_value = device.get(\"importance\", \"\")\n",
        "        # Convert to string if it's not already\n",
        "        if not isinstance(importance_value, str):\n",
        "            importance_value = str(importance_value)\n",
        "        importance = importance_value.lower()\n",
        "\n",
        "        # Generate services based on device type and importance\n",
        "        services = []\n",
        "\n",
        "        if importance in [\"critical\", \"high\"] or \"router\" in str(device_type).lower():\n",
        "            services.append({\n",
        "                \"service_id\": \"SVC001\",\n",
        "                \"name\": \"Customer Portal\",\n",
        "                \"status\": \"active\",\n",
        "                \"criticality\": \"high\"\n",
        "            })\n",
        "\n",
        "        if \"router\" in str(device_type).lower() or \"firewall\" in str(device_type).lower():\n",
        "            services.append({\n",
        "                \"service_id\": \"SVC002\",\n",
        "                \"name\": \"VPN Access\",\n",
        "                \"status\": \"active\",\n",
        "                \"criticality\": \"medium\"\n",
        "            })\n",
        "\n",
        "        if \"switch\" in str(device_type).lower() and \"distribution\" in str(device.get(\"name\", \"\")).lower():\n",
        "            services.append({\n",
        "                \"service_id\": \"SVC003\",\n",
        "                \"name\": \"Internal Applications\",\n",
        "                \"status\": \"active\",\n",
        "                \"criticality\": \"medium\"\n",
        "            })\n",
        "\n",
        "        if \"switch\" in str(device_type).lower() and \"access\" in str(device.get(\"name\", \"\")).lower():\n",
        "            services.append({\n",
        "                \"service_id\": \"SVC004\",\n",
        "                \"name\": \"Office Network\",\n",
        "                \"status\": \"active\",\n",
        "                \"criticality\": \"low\"\n",
        "            })\n",
        "\n",
        "        if \"firewall\" in str(device_type).lower():\n",
        "            services.append({\n",
        "                \"service_id\": \"SVC005\",\n",
        "                \"name\": \"Security Services\",\n",
        "                \"status\": \"active\",\n",
        "                \"criticality\": \"critical\"\n",
        "            })\n",
        "\n",
        "        return services\n",
        "\n",
        "    def format_results(self, state: DeviceSearchState) -> DeviceSearchState:\n",
        "        \"\"\"Format the results for the response\"\"\"\n",
        "        logger.info(\"Formatting results\")\n",
        "        return state\n",
        "\n",
        "    def __call__(self, input_data, mcp_context=None):\n",
        "        \"\"\"Process a device search request with improved result handling\"\"\"\n",
        "        logger.info(f\"Device search request: {input_data}\")\n",
        "\n",
        "        try:\n",
        "            # Handle different input formats\n",
        "            if isinstance(input_data, str):\n",
        "                # Direct string input\n",
        "                query = input_data\n",
        "                logger.info(f\"Direct string query: {query}\")\n",
        "            elif isinstance(input_data, dict) and \"query\" in input_data:\n",
        "                # Dictionary with query key\n",
        "                query = input_data.get(\"query\")\n",
        "                logger.info(f\"Query from dict: {query}\")\n",
        "            else:\n",
        "                # Fallback\n",
        "                query = input_data\n",
        "                logger.info(f\"Using input directly as query: {type(query)}\")\n",
        "\n",
        "            # Create initial state - DeviceSearchState will handle conversion\n",
        "            initial_state = DeviceSearchState(query=query)\n",
        "\n",
        "            # Run the graph\n",
        "            result = self.graph.invoke(initial_state)\n",
        "            logger.info(f\"Graph execution result type: {type(result)}\")\n",
        "\n",
        "            # Handle different result types\n",
        "            # Check if result is a dictionary-like object\n",
        "            if hasattr(result, \"get\") and callable(result.get):\n",
        "                # It's a dictionary-like object (AddableValuesDict)\n",
        "                logger.info(\"Processing result as dictionary-like object\")\n",
        "                error = result.get(\"error\")\n",
        "                formatted_result = {\n",
        "                    \"success\": error is None,\n",
        "                    \"found_devices\": result.get(\"found_devices\", []),\n",
        "                    \"upstream_devices\": result.get(\"upstream_devices\", {}),\n",
        "                    \"downstream_devices\": result.get(\"downstream_devices\", {}),\n",
        "                    \"affected_services\": result.get(\"affected_services\", {}),\n",
        "                }\n",
        "                if error:\n",
        "                    formatted_result[\"error\"] = error\n",
        "            else:\n",
        "                # It should be a DeviceSearchState object\n",
        "                logger.info(\"Processing result as DeviceSearchState object\")\n",
        "                formatted_result = {\n",
        "                    \"success\": not hasattr(result, \"error\") or result.error is None,\n",
        "                    \"found_devices\": getattr(result, \"found_devices\", []),\n",
        "                    \"upstream_devices\": getattr(result, \"upstream_devices\", {}),\n",
        "                    \"downstream_devices\": getattr(result, \"downstream_devices\", {}),\n",
        "                    \"affected_services\": getattr(result, \"affected_services\", {}),\n",
        "                }\n",
        "                if hasattr(result, \"error\") and result.error:\n",
        "                    formatted_result[\"error\"] = result.error\n",
        "\n",
        "            return formatted_result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in device search agent: {str(e)}\")\n",
        "            import traceback\n",
        "            logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": f\"Error processing query: {str(e)}\",\n",
        "                \"found_devices\": []\n",
        "            }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBSNngm1FRVK"
      },
      "source": [
        "### Creating an Agentic RAG System - Specialized Agents Accomplish Tasks based on their domain expertise/definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYidr2AZMSEQ",
        "outputId": "45cc65fc-fd30-4c7a-d119-3b8ac24aa815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing rag_system_updated.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile rag_system_updated.py\n",
        "import os\n",
        "import logging\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Configure logging with more detail\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(\"rag_system\")\n",
        "\n",
        "# Define the Path for Knowledge Base for each of the defined agents for Embeddings , Vector store and Retrieval\n",
        "class NetworkRAGSystem:\n",
        "    def __init__(self, knowledge_base_path=\"/content/agentic_rag-mcp_system/network_knowledge_base.txt\"):\n",
        "        self.doc_collections = {}\n",
        "        self.vector_stores = {}\n",
        "        self.llm = None\n",
        "        self.initialized = False\n",
        "        self.knowledge_base_path = knowledge_base_path\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"Initialize the RAG system with better error handling\"\"\"\n",
        "        if self.initialized:\n",
        "            return True\n",
        "\n",
        "        logger.info(\"Initializing RAG system...\")\n",
        "\n",
        "        # Test section mapping\n",
        "        test_sections = [\"TROUBLESHOOTING\", \"OBSERVABILITY\", \"DEVICE SEARCH\", \"KNOWLEDGE BASE\", \"INCIDENT\", \"DEVICE INVENTORY\"]\n",
        "        for section in test_sections:\n",
        "            mapped = self._map_section_to_agent(section)\n",
        "            logger.info(f\"Test mapping: '{section}' → '{mapped}'\")\n",
        "\n",
        "        # Sequential initialization with verification\n",
        "        if not self._load_documents():\n",
        "            logger.error(\"Document loading failed, initialization incomplete\")\n",
        "            return False\n",
        "\n",
        "        if not self._create_vector_stores():\n",
        "            logger.error(\"Vector store creation failed, initialization incomplete\")\n",
        "            return False\n",
        "\n",
        "        if not self._init_llm():\n",
        "            logger.error(\"LLM initialization failed, initialization incomplete\")\n",
        "            return False\n",
        "\n",
        "        self.initialized = True\n",
        "        logger.info(\"RAG system successfully initialized\")\n",
        "        return True\n",
        "\n",
        "    def _load_documents(self):\n",
        "        \"\"\"Load documents from file with enhanced validation and error reporting\"\"\"\n",
        "        logger.info(f\"Loading documents from {self.knowledge_base_path}...\")\n",
        "\n",
        "        # Verify file exists\n",
        "        kb_path = Path(self.knowledge_base_path)\n",
        "        if not kb_path.exists():\n",
        "            logger.error(f\"Knowledge base file not found at {kb_path.absolute()}\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            with open(kb_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                content = f.read()\n",
        "\n",
        "            if not content:\n",
        "                logger.error(\"Knowledge base file is empty\")\n",
        "                return False\n",
        "\n",
        "            logger.info(f\"Successfully read knowledge base file, size: {len(content)} bytes\")\n",
        "\n",
        "            # Print raw section names for debugging\n",
        "            section_matches = re.findall(r'# (\\w+(?:\\s+\\w+)*) DOCUMENTS #', content)\n",
        "            logger.info(f\"Found raw section names: {section_matches}\")\n",
        "\n",
        "            # Split content into document collections by section\n",
        "            section_pattern = r'# (\\w+(?:\\s+\\w+)*) DOCUMENTS #'\n",
        "            sections = re.split(section_pattern, content)\n",
        "\n",
        "            if len(sections) <= 1:\n",
        "                logger.error(\"Could not parse any sections from knowledge base\")\n",
        "                return False\n",
        "\n",
        "            logger.info(f\"Found {(len(sections)-1)//2} sections in knowledge base\")\n",
        "\n",
        "            # Process each section (odd indices are section names, even indices are content)\n",
        "            for i in range(1, len(sections), 2):\n",
        "                if i+1 < len(sections):\n",
        "                    section_name = sections[i].strip()\n",
        "                    section_content = sections[i+1].strip()\n",
        "                    logger.info(f\"Processing section: {section_name}, content length: {len(section_content)}\")\n",
        "\n",
        "                    # Extract documents (lines starting with \"DOCUMENT X:\")\n",
        "                    docs = []\n",
        "                    for doc in re.split(r'DOCUMENT \\w+:', section_content):\n",
        "                        if doc.strip():\n",
        "                            docs.append(doc.strip())\n",
        "\n",
        "                    if not docs:\n",
        "                        logger.warning(f\"No documents extracted from section {section_name}\")\n",
        "                        continue\n",
        "\n",
        "                    # Map section names to agent types\n",
        "                    agent_type = self._map_section_to_agent(section_name)\n",
        "                    logger.info(f\"Mapped section '{section_name}' to agent type '{agent_type}'\")\n",
        "\n",
        "                    # Store in document collections\n",
        "                    self.doc_collections[agent_type] = docs\n",
        "                    logger.info(f\"Loaded {len(docs)} documents for {agent_type} agent\")\n",
        "\n",
        "                    # Print first few chars of first doc for verification\n",
        "                    if docs:\n",
        "                        logger.info(f\"Sample doc for {agent_type}: {docs[0][:100]}...\")\n",
        "\n",
        "            # Verify -  loaded at least some documents\n",
        "            if not self.doc_collections:\n",
        "                logger.error(\"No documents were successfully loaded\")\n",
        "                return False\n",
        "\n",
        "            logger.info(f\"Successfully loaded documents for {list(self.doc_collections.keys())}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading documents: {str(e)}\")\n",
        "            import traceback\n",
        "            logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
        "            return False\n",
        "\n",
        "    def _map_section_to_agent(self, section_name):\n",
        "        \"\"\"Map section names to agent types with improved matching\"\"\"\n",
        "        # Normalize section name\n",
        "        section_name = section_name.lower().strip()\n",
        "\n",
        "        # Direct mappings for commonly used section names\n",
        "        direct_mappings = {\n",
        "            \"device inventory\": \"device_inventory\",\n",
        "            \"device search\": \"device_search\",\n",
        "            \"troubleshooting\": \"troubleshooting\",\n",
        "            \"observability\": \"observability\",\n",
        "            \"knowledge base\": \"knowledge_base\",\n",
        "            \"incident resolution\": \"incident_resolution\",\n",
        "            \"incident\": \"incident_resolution\"\n",
        "        }\n",
        "\n",
        "        # Check for exact match first\n",
        "        if section_name in direct_mappings:\n",
        "            return direct_mappings[section_name]\n",
        "\n",
        "        # Keyword-based mapping as fallback\n",
        "        mappings = {\n",
        "            \"troubleshooting\": [\"troubleshoot\", \"trouble\", \"issue\", \"problem\", \"error\", \"diagnos\"],\n",
        "            \"observability\": [\"observ\", \"monitor\", \"metric\", \"alert\", \"threshold\", \"capac\", \"trend\"],\n",
        "            \"device_search\": [\"device search\", \"topolog\", \"network\", \"infrastructure\", \"dependencies\"],\n",
        "            \"device_inventory\": [\"inventory\", \"device inventory\", \"equipment\", \"assets\", \"ci\"],\n",
        "            \"knowledge_base\": [\"knowledge\", \"protocol\", \"secur\", \"best\", \"practice\", \"reference\", \"guide\"],\n",
        "            \"incident_resolution\": [\"incident\", \"sever\", \"resolut\", \"response\", \"communication\"]\n",
        "        }\n",
        "\n",
        "        # Check each agent type's keyword list\n",
        "        for agent_type, keywords in mappings.items():\n",
        "            for keyword in keywords:\n",
        "                if keyword in section_name:\n",
        "                    logger.info(f\"Mapped section '{section_name}' to agent '{agent_type}' via keyword '{keyword}'\")\n",
        "                    return agent_type\n",
        "\n",
        "        # Default mapping\n",
        "        logger.warning(f\"No mapping found for section '{section_name}', using default mapping\")\n",
        "        if \"device\" in section_name:\n",
        "            return \"device_search\"\n",
        "        return \"knowledge_base\"  # Better default than \"general\"\n",
        "\n",
        "    def _create_vector_stores(self):\n",
        "        \"\"\"Create vector stores with validation and diagnostics\"\"\"\n",
        "        logger.info(\"Creating vector stores...\")\n",
        "\n",
        "        if not self.doc_collections:\n",
        "            logger.error(\"No document collections available to index\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Initialize embeddings model with validation\n",
        "            try:\n",
        "                embeddings = HuggingFaceEmbeddings(\n",
        "                    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "                )\n",
        "                # Quick test to verify embeddings work\n",
        "                test_embedding = embeddings.embed_query(\"test\")\n",
        "                if not test_embedding or len(test_embedding) == 0:\n",
        "                    raise ValueError(\"Embedding model returned empty embeddings\")\n",
        "                logger.info(f\"Embeddings validated, dimension: {len(test_embedding)}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to initialize embeddings model: {e}\")\n",
        "                return False\n",
        "\n",
        "            # Create text splitter\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=500,\n",
        "                chunk_overlap=50\n",
        "            )\n",
        "\n",
        "            # Process each document collection\n",
        "            successful_stores = 0\n",
        "            for agent_type, docs in self.doc_collections.items():\n",
        "                texts = []\n",
        "                metadatas = []\n",
        "\n",
        "                # Split documents into chunks\n",
        "                for i, doc in enumerate(docs):\n",
        "                    chunks = text_splitter.split_text(doc)\n",
        "                    logger.info(f\"Split document {i+1} for {agent_type} into {len(chunks)} chunks\")\n",
        "                    for j, chunk in enumerate(chunks):\n",
        "                        texts.append(chunk)\n",
        "                        metadatas.append({\n",
        "                            \"source\": f\"Document {i+1} for {agent_type}\",\n",
        "                            \"agent_type\": agent_type,\n",
        "                            \"chunk_id\": j\n",
        "                        })\n",
        "\n",
        "                # Create vector store if we have documents\n",
        "                if texts:\n",
        "                    logger.info(f\"Creating vector store for {agent_type} with {len(texts)} chunks\")\n",
        "                    try:\n",
        "                        vector_store = Chroma.from_texts(\n",
        "                            texts=texts,\n",
        "                            embedding=embeddings,\n",
        "                            metadatas=metadatas\n",
        "                        )\n",
        "\n",
        "                        # Validate the vector store with a simple query\n",
        "                        test_results = vector_store.similarity_search(f\"test query for {agent_type}\", k=1)\n",
        "                        if len(test_results) > 0:\n",
        "                            logger.info(f\"Vector store for {agent_type} validated\")\n",
        "                            self.vector_stores[agent_type] = vector_store\n",
        "                            successful_stores += 1\n",
        "                        else:\n",
        "                            logger.error(f\"Vector store for {agent_type} failed validation check\")\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Error creating vector store for {agent_type}: {e}\")\n",
        "                else:\n",
        "                    logger.warning(f\"No text chunks generated for {agent_type}\")\n",
        "\n",
        "            # Verify at least some vector stores were created\n",
        "            if successful_stores == 0:\n",
        "                logger.error(\"No vector stores were successfully created\")\n",
        "                return False\n",
        "\n",
        "            logger.info(f\"Successfully created {successful_stores} vector stores: {list(self.vector_stores.keys())}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in vector store creation: {str(e)}\")\n",
        "            import traceback\n",
        "            logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
        "            return False\n",
        "\n",
        "    def _init_llm(self):\n",
        "        \"\"\"Initialize the language model with proper token limits\"\"\"\n",
        "        logger.info(\"Loading language model...\")\n",
        "\n",
        "        try:\n",
        "            model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_id,\n",
        "                torch_dtype=\"auto\",\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "\n",
        "            # Fix: Use max_new_tokens instead of max_length\n",
        "            pipe = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                max_new_tokens=512,  # Allow generating up to 512 new tokens\n",
        "                do_sample=True,      # Enable sampling\n",
        "                temperature=0.7,\n",
        "                top_p=0.95,\n",
        "                repetition_penalty=1.1\n",
        "            )\n",
        "\n",
        "            self.llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "            # Validate the LLM with a simple query\n",
        "            test_response = self.llm.invoke(\"Test query to verify model is working.\")\n",
        "            if not test_response:\n",
        "                logger.error(\"LLM validation failed - empty response\")\n",
        "                return False\n",
        "\n",
        "            logger.info(\"LLM initialized and validated\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing LLM: {str(e)}\")\n",
        "            import traceback\n",
        "            logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
        "\n",
        "            # Create a simple mock LLM for testing\n",
        "            from langchain.llms.fake import FakeListLLM\n",
        "            responses = [\n",
        "                \"This is a mock response because the real LLM could not be initialized. Please check the logs for details.\"\n",
        "            ]\n",
        "            self.llm = FakeListLLM(responses=responses)\n",
        "            logger.warning(\"Using mock LLM due to initialization error\")\n",
        "            return False\n",
        "\n",
        "    def _query_all_stores(self, query_text, k=3):\n",
        "        \"\"\"Query all available vector stores when the specific one isn't found\"\"\"\n",
        "        logger.info(f\"Performing cross-store query: {query_text}\")\n",
        "\n",
        "        all_docs = []\n",
        "        all_sources = []\n",
        "\n",
        "        # Sort stores by potential relevance (device queries should check device-related stores first)\n",
        "        prioritized_stores = sorted(\n",
        "            self.vector_stores.items(),\n",
        "            key=lambda x: 1 if \"device\" in x[0] else 2  # Prioritize device-related stores\n",
        "        )\n",
        "\n",
        "        for store_name, vector_store in prioritized_stores:\n",
        "            try:\n",
        "                logger.info(f\"Querying '{store_name}' store\")\n",
        "                docs = vector_store.similarity_search(query_text, k=k)\n",
        "                if docs:\n",
        "                    logger.info(f\"Found {len(docs)} docs in '{store_name}' store\")\n",
        "                    all_docs.extend(docs)\n",
        "                    all_sources.extend([doc.metadata.get(\"source\", f\"Unknown from {store_name}\") for doc in docs])\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error querying {store_name} store: {e}\")\n",
        "\n",
        "        # Sort by relevance (simplistic approach)\n",
        "        return all_docs[:k*2], all_sources[:k*2]  # Return more than k to allow filtering\n",
        "\n",
        "    def query(self, agent_type, query_text, k=3, fallback_to_general=True):\n",
        "        \"\"\"Query the RAG system with enhanced retrieval and cross-store fallback\"\"\"\n",
        "        if not self.initialized:\n",
        "            logger.info(\"System not initialized, initializing now...\")\n",
        "            if not self.initialize():\n",
        "                return {\n",
        "                    \"response\": \"The RAG system failed to initialize properly. Please check the logs.\",\n",
        "                    \"sources\": [],\n",
        "                    \"retrieved_content\": []\n",
        "                }\n",
        "\n",
        "        # Add diagnostic logging\n",
        "        logger.info(f\"Available vector stores: {list(self.vector_stores.keys())}\")\n",
        "        logger.info(f\"Available document collections: {list(self.doc_collections.keys())}\")\n",
        "\n",
        "        # Handle case where query_text might be a dictionary\n",
        "        if isinstance(query_text, dict):\n",
        "            if \"description\" in query_text:\n",
        "                query_text = query_text[\"description\"]\n",
        "            else:\n",
        "                query_text = str(query_text)\n",
        "\n",
        "        # Preprocess query\n",
        "        query_text = query_text.strip().lower()\n",
        "\n",
        "        logger.info(f\"RAG query for {agent_type}: {query_text}\")\n",
        "\n",
        "        # Get the vector store for this agent\n",
        "        vector_store = self.vector_stores.get(agent_type)\n",
        "        docs = []\n",
        "        sources = []\n",
        "\n",
        "        if not vector_store:\n",
        "            logger.warning(f\"No vector store available for {agent_type}\")\n",
        "\n",
        "            # Try cross-store searching first\n",
        "            logger.info(\"Attempting cross-store search\")\n",
        "            docs, sources = self._query_all_stores(query_text, k=k)\n",
        "\n",
        "            if not docs and fallback_to_general:\n",
        "                # If cross-store search fails, try the traditional fallbacks\n",
        "                if \"knowledge_base\" in self.vector_stores:\n",
        "                    logger.info(f\"Falling back to 'knowledge_base' vector store\")\n",
        "                    agent_type = \"knowledge_base\"\n",
        "                    vector_store = self.vector_stores[\"knowledge_base\"]\n",
        "                elif self.vector_stores:\n",
        "                    # Last resort - use any available store\n",
        "                    fallback_agent = next(iter(self.vector_stores.keys()))\n",
        "                    logger.info(f\"Falling back to '{fallback_agent}' vector store\")\n",
        "                    agent_type = fallback_agent\n",
        "                    vector_store = self.vector_stores[fallback_agent]\n",
        "                else:\n",
        "                    logger.error(\"No vector stores available\")\n",
        "                    return {\n",
        "                        \"response\": f\"I don't have specific knowledge for this query type: {agent_type}.\",\n",
        "                        \"sources\": [],\n",
        "                        \"retrieved_content\": []\n",
        "                    }\n",
        "\n",
        "        # If get docs from cross-store search, use those directly\n",
        "        if not docs and vector_store:\n",
        "            try:\n",
        "                # Retrieve relevant documents from the specific store\n",
        "                retrieval_k = k * 2  # Retrieve more docs than needed for filtering\n",
        "                logger.info(f\"Performing similarity search for {agent_type}, k={retrieval_k}\")\n",
        "                docs = vector_store.similarity_search(query_text, k=retrieval_k)\n",
        "                logger.info(f\"Retrieved {len(docs)} documents\")\n",
        "\n",
        "                # Extract sources\n",
        "                sources = [doc.metadata.get(\"source\", \"Unknown\") for doc in docs]\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in similarity search: {e}\")\n",
        "                docs = []\n",
        "                sources = []\n",
        "\n",
        "        if not docs:\n",
        "            logger.warning(f\"No documents retrieved for query: {query_text}\")\n",
        "            return {\n",
        "                \"response\": \"I couldn't find specific information to answer your query. Please try rephrasing your question.\",\n",
        "                \"sources\": [],\n",
        "                \"retrieved_content\": []\n",
        "            }\n",
        "\n",
        "        # Extract content\n",
        "        retrieved_contents = [doc.page_content for doc in docs]\n",
        "\n",
        "        # Filter to most relevant top k\n",
        "        retrieved_contents = retrieved_contents[:k]\n",
        "        sources = sources[:k]\n",
        "\n",
        "        # Log sources for debugging\n",
        "        for i, (src, content) in enumerate(zip(sources, retrieved_contents)):\n",
        "            logger.info(f\"Retrieved doc {i+1}: {src}\")\n",
        "            logger.info(f\"Content preview: {content[:100]}...\")\n",
        "\n",
        "        # Format retrieved content\n",
        "        context = \"\\n\\n\".join([f\"Content from {src}:\\n{content}\" for src, content in zip(sources, retrieved_contents)])\n",
        "\n",
        "        # Create agent-specific prompts\n",
        "        if agent_type == \"troubleshooting\":\n",
        "            system_prompt = \"You are a network troubleshooting expert. Use the retrieved information to diagnose the problem.\"\n",
        "            prompt_template = \"\"\"<|im_start|>system\n",
        "{system_prompt}\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "I need help troubleshooting this network issue:\n",
        "{query}\n",
        "\n",
        "Here is relevant information from our knowledge base:\n",
        "{context}\n",
        "\n",
        "Based on this information, please provide:\n",
        "1. An analysis of the likely root cause\n",
        "2. Possible contributing factors\n",
        "3. Recommended troubleshooting steps\n",
        "4. Priority assessment\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "        elif agent_type == \"device_search\" or agent_type == \"device_inventory\":\n",
        "            system_prompt = \"You are a network topology expert. Help identify device information and relationships.\"\n",
        "            prompt_template = \"\"\"<|im_start|>system\n",
        "{system_prompt}\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "I need information about network devices:\n",
        "{query}\n",
        "\n",
        "Here is relevant information about our network devices:\n",
        "{context}\n",
        "\n",
        "Based on this information, please provide:\n",
        "1. Detailed device information that matches the query\n",
        "2. The importance of these devices in the network\n",
        "3. Any relationships or dependencies with other devices\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "        else:\n",
        "            # Generic prompt for other agent types\n",
        "            system_prompt = \"You are a network expert. Provide information based on the retrieved knowledge.\"\n",
        "            prompt_template = \"\"\"<|im_start|>system\n",
        "{system_prompt}\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "{query}\n",
        "\n",
        "Here is relevant information:\n",
        "{context}\n",
        "\n",
        "Please provide a helpful response based on this information.\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "        # Format the prompt\n",
        "        prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "        formatted_prompt = prompt.format(\n",
        "            system_prompt=system_prompt,\n",
        "            query=query_text,\n",
        "            context=context\n",
        "        )\n",
        "\n",
        "        # Generate response\n",
        "        try:\n",
        "            response = self.llm.invoke(formatted_prompt)\n",
        "\n",
        "            # Return results\n",
        "            return {\n",
        "                \"response\": response,\n",
        "                \"sources\": sources,\n",
        "                \"retrieved_content\": retrieved_contents\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating response: {e}\")\n",
        "            import traceback\n",
        "            logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
        "\n",
        "            return {\n",
        "                \"response\": f\"I retrieved relevant information but encountered an error when generating a response: {str(e)}\",\n",
        "                \"sources\": sources,\n",
        "                \"retrieved_content\": retrieved_contents\n",
        "            }\n",
        "\n",
        "# Function used in gradio_app.py to initialize the LLM for LangGraph\n",
        "def init_llm_for_langgraph():\n",
        "    \"\"\"Loading TinyLlama model for LangGraph with proper token limits\"\"\"\n",
        "    logger.info(\"Loading TinyLlama model for LangGraph...\")\n",
        "\n",
        "    try:\n",
        "        model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=\"auto\",\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        # Use max_new_tokens instead of max_length to avoid error while processing\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=512,  # Allow generating up to 512 new tokens\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "        return HuggingFacePipeline(pipeline=pipe)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error initializing LLM: {e}\")\n",
        "\n",
        "        # Create a simple mock LLM for testing\n",
        "        from langchain.llms.fake import FakeListLLM\n",
        "        responses = [\n",
        "            \"This is a mock response for testing purposes. The real LLM could not be initialized.\"\n",
        "        ]\n",
        "        logger.warning(\"Using mock LLM due to initialization error\")\n",
        "        return FakeListLLM(responses=responses)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA6u5kRvJURS"
      },
      "source": [
        "### Creating The AI Chatbot\n",
        "- Our UI is developed using Gradio for interation with Data through MCP -> LLM ->RAG/LangGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXigTOGkMR_1",
        "outputId": "ffc8d949-d259-4c31-e47a-50a9c45be455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing update_gradio_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile update_gradio_app.py\n",
        "import gradio as gr\n",
        "import json\n",
        "import logging\n",
        "import re\n",
        "from rag_system_updated import NetworkRAGSystem\n",
        "from full_mcp_implementation import create_mcp_system, AgentContext\n",
        "from langgraph_device_search_agent import LangGraphDeviceSearchAgent\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"gradio_app\")\n",
        "\n",
        "# Initialize the RAG system with correct path\n",
        "rag_system = NetworkRAGSystem(knowledge_base_path=\"/content/agentic_rag-mcp_system/network_knowledge_base.txt\")\n",
        "\n",
        "# Initialize the LLM for LangGraph\n",
        "def init_llm_for_langgraph():\n",
        "    logger.info(\"Loading TinyLlama model for LangGraph...\")\n",
        "\n",
        "    try:\n",
        "        model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=\"auto\",\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        # Use max_new_tokens instead of max_length\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=512,  # Allow generating up to 512 new tokens\n",
        "            do_sample=True,      # Enable sampling\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "        return HuggingFacePipeline(pipeline=pipe)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error initializing LLM: {e}\")\n",
        "\n",
        "        # Create a simple mock LLM for testing\n",
        "        from langchain.llms.fake import FakeListLLM\n",
        "        responses = [\n",
        "            \"This is a mock response for testing purposes. The real LLM could not be initialized.\"\n",
        "        ]\n",
        "        logger.warning(\"Using mock LLM due to initialization error\")\n",
        "        return FakeListLLM(responses=responses)\n",
        "\n",
        "# Initialize LangGraph Device Search Agent\n",
        "llm = init_llm_for_langgraph()\n",
        "device_search_agent = LangGraphDeviceSearchAgent(llm=llm, rag_system=rag_system)\n",
        "\n",
        "# Initialize the MCP system with LangGraph agent\n",
        "mcp_registry = create_mcp_system(rag_system, device_search_agent)\n",
        "\n",
        "# Create a shared context that persists across interactions\n",
        "shared_context = AgentContext()\n",
        "\n",
        "def initialize_system():\n",
        "    \"\"\"Initialize the RAG system when the app loads\"\"\"\n",
        "    logger.info(\"Initializing system...\")\n",
        "    success = rag_system.initialize()\n",
        "    if success:\n",
        "        logger.info(\"System initialized successfully\")\n",
        "    else:\n",
        "        logger.error(\"System initialization failed\")\n",
        "    return success\n",
        "\n",
        "def format_sources(sources):\n",
        "    \"\"\"Format source information for display\"\"\"\n",
        "    if not sources:\n",
        "        return \"No sources used\"\n",
        "\n",
        "    return \"Sources:\\n\" + \"\\n\".join([f\"- {source}\" for source in sources])\n",
        "\n",
        "# Defining Interfaces for each Agent on Gradio\n",
        "\n",
        "def device_search_interface(query):\n",
        "    \"\"\"Handle device search queries using MCP with consistent field handling\"\"\"\n",
        "    logger.info(f\"Processing device search query: {query}\")\n",
        "\n",
        "    try:\n",
        "        # Process with MCP system, specifying the agent type explicitly\n",
        "        result = mcp_registry.process_query(query, agent_type=\"device_search\", context=shared_context)\n",
        "\n",
        "        # Debug: Log the exact device structure we're receiving\n",
        "        logger.info(f\"Device search result structure: {list(result.keys())}\")\n",
        "\n",
        "        if \"error\" in result:\n",
        "            error_msg = result.get(\"error\", \"Unknown error\")\n",
        "            return f\"## Device Search Results\\nThe search could not be completed: {error_msg}\\n\\nNo sources used.\"\n",
        "\n",
        "        # Format the found devices\n",
        "        output = \"## Device Search Results\\n\\n\"\n",
        "\n",
        "        # Determine which format we're dealing with and extract devices\n",
        "        devices = []\n",
        "        if \"devices\" in result:\n",
        "            devices = result.get(\"devices\", [])\n",
        "        elif \"found_devices\" in result:\n",
        "            devices = result.get(\"found_devices\", [])\n",
        "\n",
        "        if devices:\n",
        "            output += f\"### Found {len(devices)} Devices\\n\\n\"\n",
        "            for device in devices:\n",
        "                # Normalize keys to lowercase for consistent access\n",
        "                device_norm = {k.lower(): v for k, v in device.items()}\n",
        "\n",
        "                # Extract fields with fallbacks\n",
        "                device_name = device_norm.get('name', device_norm.get('description', 'Unknown Device'))\n",
        "                device_type = device_norm.get('ci_type', device_norm.get('type', 'Unknown'))\n",
        "                device_id = device_norm.get('ci_id', device_norm.get('id', 'Unknown'))\n",
        "                device_status = device_norm.get('status', device_norm.get('state', 'Unknown'))\n",
        "                device_location = device_norm.get('location', device_norm.get('site', ''))\n",
        "                device_importance = device_norm.get('importance', device_norm.get('criticality', ''))\n",
        "\n",
        "                # Format device information\n",
        "                output += f\"- **{device_name}** ({device_type})\\n\"\n",
        "                output += f\"  - ID: {device_id}\\n\"\n",
        "                output += f\"  - Status: {device_status}\\n\"\n",
        "                if device_location:\n",
        "                    output += f\"  - Location: {device_location}\\n\"\n",
        "                if device_importance:\n",
        "                    output += f\"  - Importance: {device_importance}\\n\"\n",
        "                output += \"\\n\"\n",
        "        else:\n",
        "            output += \"No devices found matching your criteria.\\n\\n\"\n",
        "\n",
        "        # Add topology information from result\n",
        "        # First check for MCP native format\n",
        "        if \"topology_analysis\" in result:\n",
        "            topology = result.get(\"topology_analysis\", {})\n",
        "            connections = topology.get(\"connections\", [])\n",
        "            if connections:\n",
        "                output += \"### Device Connections\\n\\n\"\n",
        "                for conn in connections:\n",
        "                    output += f\"- {conn.get('from')} → {conn.get('to')} ({conn.get('type', 'connection')})\\n\"\n",
        "                output += \"\\n\"\n",
        "\n",
        "        # Then check for LangGraph format\n",
        "        upstream_devices = result.get(\"upstream_devices\", {})\n",
        "        if upstream_devices:\n",
        "            output += \"### Upstream Connections\\n\\n\"\n",
        "            for device_id, upstream_list in upstream_devices.items():\n",
        "                if upstream_list:\n",
        "                    # Find the device name with case-insensitive search\n",
        "                    device_name = device_id\n",
        "                    for d in devices:\n",
        "                        d_lower = {k.lower(): v for k, v in d.items()}\n",
        "                        if d_lower.get('ci_id', '').lower() == device_id.lower():\n",
        "                            device_name = d_lower.get('name', device_id)\n",
        "                            break\n",
        "\n",
        "                    output += f\"**{device_name}** connects to:\\n\"\n",
        "                    for upstream in upstream_list:\n",
        "                        # Case-insensitive lookup for upstream devices\n",
        "                        u_lower = {k.lower(): v for k, v in upstream.items()}\n",
        "                        up_name = u_lower.get('name', 'Unknown')\n",
        "                        up_type = u_lower.get('ci_type', u_lower.get('type', 'Unknown'))\n",
        "                        output += f\"- {up_name} ({up_type})\\n\"\n",
        "                    output += \"\\n\"\n",
        "\n",
        "        downstream_devices = result.get(\"downstream_devices\", {})\n",
        "        if downstream_devices:\n",
        "            output += \"### Downstream Connections\\n\\n\"\n",
        "            for device_id, downstream_list in downstream_devices.items():\n",
        "                if downstream_list:\n",
        "                    # Find the device name with case-insensitive search\n",
        "                    device_name = device_id\n",
        "                    for d in devices:\n",
        "                        d_lower = {k.lower(): v for k, v in d.items()}\n",
        "                        if d_lower.get('ci_id', '').lower() == device_id.lower():\n",
        "                            device_name = d_lower.get('name', device_id)\n",
        "                            break\n",
        "\n",
        "                    output += f\"**{device_name}** connects to:\\n\"\n",
        "                    for downstream in downstream_list:\n",
        "                        # Case-insensitive lookup for downstream devices\n",
        "                        d_lower = {k.lower(): v for k, v in downstream.items()}\n",
        "                        down_name = d_lower.get('name', 'Unknown')\n",
        "                        down_type = d_lower.get('ci_type', d_lower.get('type', 'Unknown'))\n",
        "                        output += f\"- {down_name} ({down_type})\\n\"\n",
        "                    output += \"\\n\"\n",
        "\n",
        "        # Add service impact information\n",
        "        affected_services = result.get(\"affected_services\", {})\n",
        "        if affected_services:\n",
        "            output += \"### Affected Services\\n\\n\"\n",
        "            all_services = []\n",
        "            for device_id, service_list in affected_services.items():\n",
        "                all_services.extend(service_list)\n",
        "\n",
        "            # Deduplicate services\n",
        "            unique_services = {}\n",
        "            for service in all_services:\n",
        "                service_id = service.get(\"service_id\")\n",
        "                if service_id and service_id not in unique_services:\n",
        "                    unique_services[service_id] = service\n",
        "\n",
        "            for service in unique_services.values():\n",
        "                # Case-insensitive lookup for services\n",
        "                s_lower = {k.lower(): v for k, v in service.items()}\n",
        "                svc_name = s_lower.get('name', 'Unknown')\n",
        "                svc_crit = s_lower.get('criticality', 'Unknown')\n",
        "                output += f\"- **{svc_name}** (Criticality: {svc_crit})\\n\"\n",
        "\n",
        "        # Add sources\n",
        "        sources = result.get(\"sources\", [])\n",
        "        if sources:\n",
        "            output += f\"\\n## {format_sources(sources)}\"\n",
        "\n",
        "        # Add MCP agent attribution\n",
        "        output += f\"\\n\\n*Processed by {result.get('agent_type', 'Unknown')} agent ({result.get('agent_id', 'Unknown')})*\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in device search interface: {e}\")\n",
        "        import traceback\n",
        "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
        "        return f\"## Device Search Results\\nError processing query: {str(e)}\\n\\nNo sources used.\"\n",
        "\n",
        "def troubleshooting_interface(issue_description, related_cis, logs):\n",
        "    \"\"\"Handle troubleshooting queries using MCP\"\"\"\n",
        "    # Combine inputs into a single query\n",
        "    query = f\"Issue: {issue_description}\\n\"\n",
        "    if related_cis:\n",
        "        query += f\"Related devices: {related_cis}\\n\"\n",
        "    if logs:\n",
        "        query += f\"Logs: {logs}\\n\"\n",
        "\n",
        "    # Process with MCP system\n",
        "    result = mcp_registry.process_query(query, agent_type=\"troubleshooting\", context=shared_context)\n",
        "\n",
        "    # Format the response\n",
        "    if \"analysis\" in result:\n",
        "        response = result[\"analysis\"]\n",
        "        suggested_actions = result.get(\"suggested_actions\", [])\n",
        "        action_text = \"\\n\".join([f\"- {action}\" for action in suggested_actions]) if suggested_actions else \"\"\n",
        "\n",
        "        sources = format_sources(result.get(\"sources\", []))\n",
        "\n",
        "        # Format output\n",
        "        output = f\"## Analysis\\n{response}\\n\\n\"\n",
        "        if action_text:\n",
        "            output += f\"## Suggested Actions\\n{action_text}\\n\\n\"\n",
        "        output += f\"## {sources}\"\n",
        "\n",
        "        # Add MCP agent attribution\n",
        "        output += f\"\\n\\n*Processed by {result.get('agent_type', 'Unknown')} agent ({result.get('agent_id', 'Unknown')})*\"\n",
        "    else:\n",
        "        output = f\"## Error\\n{result.get('error', 'Unknown error occurred')}\"\n",
        "\n",
        "    return output\n",
        "\n",
        "def knowledge_base_interface(query, doc_type):\n",
        "    \"\"\"Handle knowledge base queries using MCP\"\"\"\n",
        "    # Add document type to query if specified\n",
        "    if doc_type and doc_type != \"All Types\":\n",
        "        query = f\"{query} (document type: {doc_type})\"\n",
        "\n",
        "    # Process with MCP system\n",
        "    result = mcp_registry.process_query(query, agent_type=\"knowledge_base\", context=shared_context)\n",
        "\n",
        "    # Format the response\n",
        "    if \"answer\" in result:\n",
        "        response = result[\"answer\"]\n",
        "        sources = format_sources(result.get(\"sources\", []))\n",
        "        related_topics = result.get(\"related_topics\", [])\n",
        "\n",
        "        # Format output\n",
        "        output = f\"## Knowledge Base Information\\n{response}\\n\\n\"\n",
        "\n",
        "        if related_topics:\n",
        "            topic_text = \", \".join(related_topics)\n",
        "            output += f\"## Related Topics\\n{topic_text}\\n\\n\"\n",
        "\n",
        "        output += f\"## {sources}\"\n",
        "\n",
        "        # Add MCP agent attribution\n",
        "        output += f\"\\n\\n*Processed by {result.get('agent_type', 'Unknown')} agent ({result.get('agent_id', 'Unknown')})*\"\n",
        "    else:\n",
        "        output = f\"## Error\\n{result.get('error', 'Unknown error occurred')}\"\n",
        "\n",
        "    return output\n",
        "\n",
        "def observability_interface(ci_types, metrics, time_range):\n",
        "    \"\"\"Handle observability queries using MCP\"\"\"\n",
        "    # Combine inputs into a single query\n",
        "    query = f\"I need to analyze metrics for CI types: {ci_types}, focusing on these metrics: {metrics}, over time range: {time_range}\"\n",
        "\n",
        "    # Process with MCP system\n",
        "    result = mcp_registry.process_query(query, agent_type=\"observability\", context=shared_context)\n",
        "\n",
        "    # Format the response\n",
        "    if \"assessment\" in result:\n",
        "        response = result[\"assessment\"]\n",
        "        sources = format_sources(result.get(\"sources\", []))\n",
        "\n",
        "        # Format output\n",
        "        output = f\"## Network Health Assessment\\n{response}\\n\\n\"\n",
        "        output += f\"## {sources}\"\n",
        "\n",
        "        # Add MCP agent attribution\n",
        "        output += f\"\\n\\n*Processed by {result.get('agent_type', 'Unknown')} agent ({result.get('agent_id', 'Unknown')})*\"\n",
        "    else:\n",
        "        output = f\"## Error\\n{result.get('error', 'Unknown error occurred')}\"\n",
        "\n",
        "    return output\n",
        "\n",
        "def incident_resolution_interface(incident_id, title, description, status, priority, affected_cis):\n",
        "    \"\"\"Handle incident resolution queries using MCP\"\"\"\n",
        "    # Combine inputs into a single query\n",
        "    query = f\"Incident ID: {incident_id}\\nTitle: {title}\\nDescription: {description}\\nStatus: {status}\\nPriority: {priority}\\nAffected CIs: {affected_cis}\"\n",
        "\n",
        "    # Process with MCP system\n",
        "    result = mcp_registry.process_query(query, agent_type=\"incident_resolution\", context=shared_context)\n",
        "\n",
        "    # Format the response\n",
        "    if \"summary\" in result:\n",
        "        response = result[\"summary\"]\n",
        "        sources = format_sources(result.get(\"sources\", []))\n",
        "        action_items = result.get(\"action_items\", [])\n",
        "\n",
        "        # Format output\n",
        "        output = f\"## Incident Resolution\\n{response}\\n\\n\"\n",
        "\n",
        "        if action_items:\n",
        "            output += \"## Action Items\\n\"\n",
        "            for i, item in enumerate(action_items, 1):\n",
        "                output += f\"{i}. {item.get('action')}\\n\"\n",
        "                output += f\"   - Owner: {item.get('owner')}\\n\"\n",
        "                output += f\"   - Deadline: {item.get('deadline')}\\n\\n\"\n",
        "\n",
        "        output += f\"## {sources}\"\n",
        "\n",
        "        # Add MCP agent attribution\n",
        "        output += f\"\\n\\n*Processed by {result.get('agent_type', 'Unknown')} agent ({result.get('agent_id', 'Unknown')})*\"\n",
        "    else:\n",
        "        output = f\"## Error\\n{result.get('error', 'Unknown error occurred')}\"\n",
        "\n",
        "    return output\n",
        "\n",
        "# Create the Gradio app with tabs\n",
        "with gr.Blocks(title=\"GenAI Enabled Integrated Platform Support \") as demo:\n",
        "    gr.Markdown(\"# GenAI Enabled Intergrated Platform\")\n",
        "    gr.Markdown(\"\"\"\n",
        "    This demo showcases a complete Model Context Protocol (MCP) implementation with RAG-enhanced agents.\n",
        "    The system maintains context across different agent interactions.The Power of Agentic RAG and LangGraph for efficient information retrieval.\n",
        "    \"\"\")\n",
        "\n",
        "    # Initialize the system when loading\n",
        "    system_initialized = gr.Checkbox(value=False, visible=False, label=\"System Initialized\")\n",
        "\n",
        "    demo.load(initialize_system, None, system_initialized)\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.Tab(\"Device Search\"):\n",
        "            with gr.Group():\n",
        "                search_query = gr.Textbox(label=\"Search Query\", placeholder=\"e.g., 'Find all routers in NYC' or 'Show critical devices'\")\n",
        "                search_btn = gr.Button(\"Search Devices\")\n",
        "                search_output = gr.Markdown(label=\"Search Results\")\n",
        "                search_btn.click(device_search_interface, [search_query], search_output)\n",
        "\n",
        "                gr.Markdown(\"\"\"\n",
        "                ### Example Queries\n",
        "                - \"Find all active routers\"\n",
        "                - \"core devices with high criticality\"\n",
        "                - \"Find switches in NYC\"\n",
        "                - \"Show all firewall devices\"\n",
        "                - \"Find router R001\"\n",
        "                \"\"\")\n",
        "\n",
        "        with gr.Tab(\"Troubleshooting\"):\n",
        "            with gr.Group():\n",
        "                issue_description = gr.Textbox(label=\"Issue Description\", lines=4, placeholder=\"Describe the network issue you're experiencing...\")\n",
        "                related_cis = gr.Textbox(label=\"Related Configuration Items (comma-separated)\", placeholder=\"e.g., router-01, switch-03, firewall-02\")\n",
        "                logs = gr.Textbox(label=\"Relevant Logs (optional)\", lines=4, placeholder=\"Paste any relevant log entries here...\")\n",
        "                troubleshoot_btn = gr.Button(\"Analyze Issue\")\n",
        "                troubleshoot_output = gr.Markdown(label=\"Analysis Results\")\n",
        "                troubleshoot_btn.click(troubleshooting_interface, [issue_description, related_cis, logs], troubleshoot_output)\n",
        "\n",
        "        with gr.Tab(\"Observability\"):\n",
        "            with gr.Group():\n",
        "                ci_types = gr.Textbox(label=\"CI Types to Analyze (comma-separated)\", value=\"router, switch\")\n",
        "                metrics = gr.Textbox(label=\"Metrics to Analyze (comma-separated)\", value=\"cpu_utilization, latency, memory_utilization\")\n",
        "                time_range = gr.Dropdown(label=\"Time Range\", choices=[\"last_1h\", \"last_6h\", \"last_12h\", \"last_24h\", \"last_3d\", \"last_7d\"], value=\"last_24h\")\n",
        "                observe_btn = gr.Button(\"Analyze Metrics\")\n",
        "                observe_output = gr.Markdown(label=\"Analysis Results\")\n",
        "                observe_btn.click(observability_interface, [ci_types, metrics, time_range], observe_output)\n",
        "\n",
        "        with gr.Tab(\"Knowledge Base\"):\n",
        "            with gr.Group():\n",
        "                kb_query = gr.Textbox(label=\"Search Query\", placeholder=\"e.g., 'Network latency troubleshooting' or 'Firewall best practices'\")\n",
        "                kb_doc_type = gr.Dropdown(label=\"Document Type\", choices=[\"All Types\", \"manual\", \"faq\", \"best_practice\", \"troubleshooting_guide\", \"reference\"], value=\"All Types\")\n",
        "                kb_btn = gr.Button(\"Search Knowledge Base\")\n",
        "                kb_output = gr.Markdown(label=\"Search Results\")\n",
        "                kb_btn.click(knowledge_base_interface, [kb_query, kb_doc_type], kb_output)\n",
        "\n",
        "        with gr.Tab(\"Incident Resolution\"):\n",
        "            with gr.Group():\n",
        "                inc_id = gr.Textbox(label=\"Incident ID\", value=\"INC-001\")\n",
        "                inc_title = gr.Textbox(label=\"Incident Title\", value=\"Network Outage in NYC Office\")\n",
        "                inc_description = gr.Textbox(label=\"Description\", lines=4, value=\"Users in the NYC office reported a complete loss of network connectivity at 9:15 AM.\")\n",
        "                inc_status = gr.Dropdown(label=\"Status\", choices=[\"open\", \"in_progress\", \"resolved\", \"closed\"], value=\"resolved\")\n",
        "                inc_priority = gr.Dropdown(label=\"Priority\", choices=[\"critical\", \"high\", \"medium\", \"low\"], value=\"critical\")\n",
        "                inc_cis = gr.Textbox(label=\"Affected CIs (comma-separated)\", value=\"router-nyc-01, switch-nyc-03, firewall-nyc-01\")\n",
        "                inc_btn = gr.Button(\"Generate Incident Summary\")\n",
        "                inc_output = gr.Markdown(label=\"Incident Summary\")\n",
        "                inc_btn.click(incident_resolution_interface, [inc_id, inc_title, inc_description, inc_status, inc_priority, inc_cis], inc_output)\n",
        "\n",
        "        with gr.Tab(\"About MCP Framework & LangGraph\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ## Model Context Protocol (MCP) Framework\n",
        "\n",
        "            This system demonstrates the four key elements of MCP:\n",
        "\n",
        "            ### 1. Specialized Agents\n",
        "\n",
        "            Each agent has specific expertise and capabilities:\n",
        "            - **Troubleshooting Agent**: Diagnoses network issues and suggests resolution steps\n",
        "            - **Device Search Agent**: Finds devices and analyzes topology relationships\n",
        "            - **Knowledge Base Agent**: Retrieves information and documentation\n",
        "            - **Observability Agent**: Analyzes metrics and detects anomalies\n",
        "            - **Incident Resolution Agent**: Manages incident lifecycle and provides guidance\n",
        "\n",
        "            ### 2. Context Management\n",
        "\n",
        "            The MCP framework maintains shared context between agents:\n",
        "            - Conversation history for continuity between queries\n",
        "            - Entity memory to track devices, services, and incidents\n",
        "            - Execution state to share information between agent interactions\n",
        "\n",
        "            ### 3. Agent Registry\n",
        "\n",
        "            All agents are registered in a central system:\n",
        "            - Automatic routing of queries to the most appropriate agent\n",
        "            - Capability-based discovery for specialized functions\n",
        "            - Metadata tracking for better transparency\n",
        "\n",
        "            ### 4. Knowledge Integration\n",
        "\n",
        "            Relevant knowledge is retrieved dynamically:\n",
        "            - RAG system integration for retrieving domain knowledge\n",
        "            - Context-aware queries that incorporate previous interactions\n",
        "            - Cross-agent knowledge sharing\n",
        "\n",
        "            ### How It Works\n",
        "\n",
        "            When you submit a query:\n",
        "            1. The system analyzes your query to determine the most appropriate agent\n",
        "            2. The selected agent retrieves relevant knowledge using RAG\n",
        "            3. Query context is enriched with information from previous interactions\n",
        "            4. The agent processes the query and builds a structured response\n",
        "            5. Context is updated for future interactions\n",
        "\n",
        "            ### Integration with LangGraph\n",
        "\n",
        "            The Device Search agent leverages LangGraph for a multi-step workflow:\n",
        "            1. Query parsing: Interprets intent and extracts search criteria\n",
        "            2. Device search: Finds devices matching criteria\n",
        "            3. Topology analysis: Maps relationships between devices\n",
        "            4. Results formatting: Organizes findings in a structured format\n",
        "\n",
        "            This gives you the benefits of both LangGraph's structured workflow and MCP's context sharing.\n",
        "            \"\"\")\n",
        "\n",
        "# Entry point to run the app\n",
        "if __name__ == \"__main__\":\n",
        "    # Launch with sharing enabled for Colab access\n",
        "    demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BpuCQD0K6zL"
      },
      "source": [
        "### Agent Specific Documents for RAG implementation and Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgRunyvoMR7t",
        "outputId": "5c271760-7cb2-451c-9954-d538dedbfa80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing network_knowledge_base.txt\n"
          ]
        }
      ],
      "source": [
        "# Create a document collection for all agent types\n",
        "%%writefile network_knowledge_base.txt\n",
        "\n",
        "# DEVICE INVENTORY DOCUMENTS #\n",
        "DOCUMENT INV1:\n",
        "Core Network Devices\n",
        "- Device ID: R001\n",
        "  Name: core-router-01\n",
        "  Type: router\n",
        "  Status: active\n",
        "  Location: NYC\n",
        "  Importance: critical\n",
        "  Description: Primary core router for east region datacenter\n",
        "  Connected to: WAN001, S001A, S001B\n",
        "\n",
        "- Device ID: R002\n",
        "  Name: core-router-02\n",
        "  Type: router\n",
        "  Status: active\n",
        "  Location: SFO\n",
        "  Importance: critical\n",
        "  Description: Primary core router for west region datacenter\n",
        "  Connected to: WAN002, S002A, S002B\n",
        "\n",
        "- Device ID: FW001\n",
        "  Name: edge-firewall-01\n",
        "  Type: firewall\n",
        "  Status: active\n",
        "  Location: NYC\n",
        "  Importance: critical\n",
        "  Description: Primary internet-facing firewall for east region\n",
        "  Connected to: R001, DMZ001\n",
        "\n",
        "DOCUMENT INV2:\n",
        "Distribution Network Devices\n",
        "- Device ID: S001A\n",
        "  Name: distribution-switch-01a\n",
        "  Type: switch\n",
        "  Status: active\n",
        "  Location: NYC\n",
        "  Importance: high\n",
        "  Description: Primary distribution switch for NYC east zone\n",
        "  Connected to: R001, S001A1, S001A2\n",
        "\n",
        "- Device ID: S001B\n",
        "  Name: distribution-switch-01b\n",
        "  Type: switch\n",
        "  Status: active\n",
        "  Location: NYC\n",
        "  Importance: high\n",
        "  Description: Secondary distribution switch for NYC east zone\n",
        "  Connected to: R001, S001B1, S001B2\n",
        "\n",
        "- Device ID: S002A\n",
        "  Name: distribution-switch-02a\n",
        "  Type: switch\n",
        "  Status: active\n",
        "  Location: SFO\n",
        "  Importance: high\n",
        "  Description: Primary distribution switch for SFO north zone\n",
        "  Connected to: R002, S002A1, S002A2\n",
        "\n",
        "DOCUMENT INV3:\n",
        "Access Network Devices\n",
        "- Device ID: S001A1\n",
        "  Name: access-switch-01a1\n",
        "  Type: switch\n",
        "  Status: active\n",
        "  Location: NYC-Floor1\n",
        "  Importance: medium\n",
        "  Description: Access switch for NYC east zone floor 1\n",
        "  Connected to: S001A, Various endpoints\n",
        "\n",
        "- Device ID: S001A2\n",
        "  Name: access-switch-01a2\n",
        "  Type: switch\n",
        "  Status: active\n",
        "  Location: NYC-Floor2\n",
        "  Importance: medium\n",
        "  Description: Access switch for NYC east zone floor 2\n",
        "  Connected to: S001A, Various endpoints\n",
        "\n",
        "- Device ID: S001B1\n",
        "  Name: access-switch-01b1\n",
        "  Type: switch\n",
        "  Status: warning\n",
        "  Location: NYC-Floor3\n",
        "  Importance: medium\n",
        "  Description: Access switch for NYC east zone floor 3 (showing interface errors)\n",
        "  Connected to: S001B, Various endpoints\n",
        "\n",
        "DOCUMENT INV4:\n",
        "Special Purpose Devices\n",
        "- Device ID: LB001\n",
        "  Name: load-balancer-01\n",
        "  Type: load-balancer\n",
        "  Status: active\n",
        "  Location: NYC\n",
        "  Importance: high\n",
        "  Description: F5 load balancer for customer-facing applications\n",
        "  Connected to: S001A, APP001, APP002, APP003\n",
        "\n",
        "- Device ID: WAP001\n",
        "  Name: wireless-controller-01\n",
        "  Type: wireless-controller\n",
        "  Status: active\n",
        "  Location: NYC\n",
        "  Importance: medium\n",
        "  Description: Controls all wireless access points in NYC office\n",
        "  Connected to: S001A\n",
        "\n",
        "- Device ID: DMZ001\n",
        "  Name: dmz-switch-01\n",
        "  Type: switch\n",
        "  Status: active\n",
        "  Location: NYC\n",
        "  Importance: high\n",
        "  Description: DMZ network switch\n",
        "  Connected to: FW001, WEB001, WEB002\n",
        "\n",
        "# TROUBLESHOOTING DOCUMENTS #\n",
        "\n",
        "DOCUMENT T1:\n",
        "Router High CPU Troubleshooting Guide\n",
        "When a router shows high CPU utilization (>80%), first identify the process consuming resources with 'show processes cpu'.\n",
        "Common causes include:\n",
        "1. Routing protocol misconfiguration or instability (BGP flapping, OSPF reconvergence)\n",
        "2. Access Control List (ACL) processing for high traffic volumes\n",
        "3. Network Address Translation (NAT) for numerous concurrent sessions\n",
        "4. Denial of Service (DoS) attacks or traffic anomalies\n",
        "5. Logging verbosity set too high\n",
        "Resolution steps:\n",
        "- Check for routing protocol issues with 'show ip route summary' and 'show ip protocol'\n",
        "- Review ACL configurations and consider hardware acceleration\n",
        "- Monitor interface errors with 'show interface' for physical issues\n",
        "- Implement Control Plane Policing (CoPP) for protection\n",
        "Priority: High (Critical for core routers, Medium for edge devices)\n",
        "\n",
        "DOCUMENT T2:\n",
        "Network Latency and Packet Loss Resolution\n",
        "Latency and packet loss often occur together and can severely impact application performance.\n",
        "Troubleshooting approach:\n",
        "1. Quantify the issue - use ping and traceroute to measure latency and loss percentage\n",
        "2. Isolate the problem area - test segments of the network path separately\n",
        "3. Check physical connectivity - look for interface errors, duplex mismatches\n",
        "4. Review QoS configuration - ensure proper traffic prioritization\n",
        "5. Monitor for microbursts - short traffic spikes that cause buffer overflows\n",
        "Common root causes:\n",
        "- Congestion on interfaces (check with 'show interface' for drops/discards)\n",
        "- Suboptimal routing (check with 'show ip route' and 'traceroute')\n",
        "- Faulty hardware or cabling (check error counters and interface status)\n",
        "- MTU mismatches or fragmentation issues\n",
        "Priority: High (Can significantly impact user experience)\n",
        "\n",
        "DOCUMENT T3:\n",
        "Switch Port Errors and Performance Issues\n",
        "Port errors on switches often indicate physical or data link layer problems.\n",
        "Error types and causes:\n",
        "- CRC errors: Often caused by cabling issues, EMI, or duplex mismatches\n",
        "- Collisions: Typically seen in half-duplex environments\n",
        "- Input/output errors: Can indicate buffer issues or hardware problems\n",
        "- Late collisions: Usually a sign of cable length violations or duplex mismatch\n",
        "Troubleshooting steps:\n",
        "1. Check error statistics with 'show interface' or 'show interface counters errors'\n",
        "2. Verify duplex and speed settings on both ends of the connection\n",
        "3. Test or replace cabling if physical issues are suspected\n",
        "4. Check for port utilization and buffer exhaustion\n",
        "5. Verify STP (Spanning Tree Protocol) status and configuration\n",
        "Priority: Medium (Unless affecting critical services)\n",
        "\n",
        "# OBSERVABILITY DOCUMENTS #\n",
        "\n",
        "DOCUMENT O1:\n",
        "Network Metric Baselines and Thresholds\n",
        "Understanding normal behavior is essential for effective monitoring.\n",
        "Key metrics and typical thresholds:\n",
        "- Interface utilization: Alert at 70% sustained, Critical at 90%\n",
        "- CPU utilization: Alert at 70% sustained, Critical at 90%\n",
        "- Memory utilization: Alert at 80% sustained, Critical at 95%\n",
        "- Packet loss: Alert at 1%, Critical at 5%\n",
        "- Latency (internal network): Alert at 10ms, Critical at 50ms\n",
        "- Jitter: Alert at 10ms, Critical at 30ms\n",
        "Seasonal patterns:\n",
        "- Business hours typically show 2-3x higher utilization than off-hours\n",
        "- Month-end processing may increase database traffic by 50%\n",
        "- Backup windows may saturate certain links during scheduled times\n",
        "Recommended approach: Establish baselines over 2-4 weeks of normal operation before setting thresholds.\n",
        "\n",
        "DOCUMENT O2:\n",
        "Detecting Network Performance Anomalies\n",
        "Anomaly detection requires correlation of multiple metrics.\n",
        "Common anomalies and their signatures:\n",
        "1. DDoS attacks: Sudden increase in traffic volume, connection counts, and CPU usage\n",
        "2. Routing flaps: Intermittent connectivity, increased CPU, routing protocol messages\n",
        "3. Broadcast storms: High broadcast/multicast packet counts, increased latency\n",
        "4. Link flapping: Interface status changes, routing reconvergence, packet loss\n",
        "5. Memory leaks: Gradually increasing memory usage without corresponding traffic increase\n",
        "Detection methods:\n",
        "- Rate of change is often more important than absolute values\n",
        "- Correlate metrics across multiple devices to identify systemic issues\n",
        "- Look for divergence from historical patterns at similar time periods\n",
        "- Set dynamic thresholds based on standard deviations from baseline\n",
        "\n",
        "DOCUMENT O3:\n",
        "Network Capacity Planning and Trending\n",
        "Proactive capacity management prevents performance issues.\n",
        "Key metrics to trend:\n",
        "- Interface throughput (95th percentile usage over time)\n",
        "- Growth rate of connected devices and users\n",
        "- Connection rates and session counts\n",
        "- Application-specific metrics (response times, transaction rates)\n",
        "Warning signs requiring attention:\n",
        "- Consistent growth of >5% per month on critical links\n",
        "- Latency increases during peak periods\n",
        "- Reduction in available headroom below 30%\n",
        "- Step changes in utilization patterns\n",
        "Planning guidelines:\n",
        "- Plan capacity upgrades when projected to reach 70% within 6 months\n",
        "- Consider redundancy and failover capacity in calculations\n",
        "- Account for seasonal variations and special events\n",
        "\n",
        "# DEVICE SEARCH DOCUMENTS #\n",
        "\n",
        "DOCUMENT D1:\n",
        "Network Topology Fundamentals\n",
        "Understanding dependencies between network devices is crucial for impact analysis.\n",
        "Network layers and dependencies:\n",
        "1. Core layer: High-speed backbone, minimal configuration, maximum reliability\n",
        "2. Distribution layer: Routing, filtering, QoS, and policy enforcement\n",
        "3. Access layer: End-user connectivity, security features, and service delivery\n",
        "Critical paths:\n",
        "- Default gateway dependency: End users depend on their default gateway\n",
        "- Uplink dependency: Access switches depend on distribution switches\n",
        "- Routing peer dependency: Routers depend on their routing neighbors\n",
        "Redundancy considerations:\n",
        "- Single points of failure should be identified and mitigated\n",
        "- Redundant paths should not share physical infrastructure\n",
        "- Control plane redundancy is as important as data plane redundancy\n",
        "\n",
        "DOCUMENT D2:\n",
        "Network Device Roles and Classification\n",
        "Properly classifying device roles helps with troubleshooting and change management.\n",
        "Common device roles:\n",
        "- Core router: Central backbone connectivity and routing\n",
        "- Edge router: Internet or WAN connectivity\n",
        "- Distribution switch: Aggregation and policy enforcement\n",
        "- Access switch: End-user and device connectivity\n",
        "- Firewall: Security policy enforcement and segmentation\n",
        "- Load balancer: Application traffic distribution and health monitoring\n",
        "- WAN optimizer: Traffic compression and protocol optimization\n",
        "Criticality classification:\n",
        "- Tier 1: Failure affects entire network or critical business services\n",
        "- Tier 2: Failure affects multiple departments or non-critical services\n",
        "- Tier 3: Failure affects limited number of users or services\n",
        "Change management implications:\n",
        "- Tier 1 devices require more rigorous testing and maintenance windows\n",
        "- Dependencies should be documented and considered during changes\n",
        "- Backup configurations must be verified before significant changes\n",
        "\n",
        "DOCUMENT D3:\n",
        "High-Availability Network Design Patterns\n",
        "Resilient networks require both redundancy and proper failover mechanisms.\n",
        "Key high-availability patterns:\n",
        "1. Active/Standby: Primary device with backup that takes over on failure\n",
        "2. Active/Active: Load shared across multiple devices with failover capability\n",
        "3. N+1 redundancy: Additional capacity beyond what's required for normal operation\n",
        "4. Distributed systems: Services spread across multiple devices with no single point of failure\n",
        "Implementation technologies:\n",
        "- Routing redundancy: HSRP, VRRP, GLBP for default gateway redundancy\n",
        "- Link redundancy: Equal-cost multi-path, port channels, spanning tree\n",
        "- Control plane redundancy: NSF (Non-Stop Forwarding), SSO (Stateful Switchover)\n",
        "- Device redundancy: Clustered systems, chassis redundancy, geographic distribution\n",
        "Recovery metrics:\n",
        "- RTO (Recovery Time Objective): Maximum acceptable time to restore service\n",
        "- RPO (Recovery Point Objective): Maximum acceptable data loss during recovery\n",
        "- MTTR (Mean Time To Repair): Average time to restore service after failure\n",
        "\n",
        "# KNOWLEDGE BASE DOCUMENTS #\n",
        "\n",
        "DOCUMENT K1:\n",
        "Network Protocol Troubleshooting Quick Reference\n",
        "Common protocols and troubleshooting approaches:\n",
        "1. TCP/IP:\n",
        "   - Check IP addressing and subnet configuration\n",
        "   - Verify routing tables and default gateways\n",
        "   - Test with ping, traceroute, and packet captures\n",
        "   - Common issues: Address conflicts, fragmentation, MTU issues\n",
        "2. DNS:\n",
        "   - Verify DNS server configuration\n",
        "   - Test resolution with nslookup or dig\n",
        "   - Check for latency or timeouts\n",
        "   - Common issues: Incorrect DNS servers, cache poisoning, zone transfer failures\n",
        "3. DHCP:\n",
        "   - Verify DHCP server availability\n",
        "   - Check IP address pool exhaustion\n",
        "   - Test with ipconfig /release and renew\n",
        "   - Common issues: Address conflicts, unauthorized DHCP servers, misconfigured scopes\n",
        "4. BGP:\n",
        "   - Verify neighbor relationships\n",
        "   - Check for route advertisements and withdrawals\n",
        "   - Review AS path and route selection\n",
        "   - Common issues: Peer misconfigurations, route flapping, policy errors\n",
        "5. OSPF:\n",
        "   - Verify neighbor adjacencies\n",
        "   - Check for area configuration consistency\n",
        "   - Review LSA database\n",
        "   - Common issues: Area mismatches, authentication failures, MTU mismatches\n",
        "\n",
        "DOCUMENT K2:\n",
        "Network Security Best Practices\n",
        "Essential security measures for network infrastructure:\n",
        "1. Device hardening:\n",
        "   - Disable unused services and ports\n",
        "   - Implement strong password policies\n",
        "   - Use encrypted management protocols (SSH, HTTPS)\n",
        "   - Regular patching and firmware updates\n",
        "2. Access control:\n",
        "   - Implement principle of least privilege\n",
        "   - Use AAA (Authentication, Authorization, Accounting)\n",
        "   - Secure management plane with ACLs\n",
        "   - Implement role-based access control\n",
        "3. Traffic filtering:\n",
        "   - Deploy firewalls at network boundaries\n",
        "   - Use ACLs for basic traffic filtering\n",
        "   - Implement deep packet inspection where needed\n",
        "   - Regularly audit and clean up rule sets\n",
        "4. Monitoring and detection:\n",
        "   - Enable logging and send to central collector\n",
        "   - Implement network behavior analysis\n",
        "   - Deploy IDS/IPS systems at critical points\n",
        "   - Regular review of security events\n",
        "5. Incident response:\n",
        "   - Documented procedures for common security incidents\n",
        "   - Ability to quickly implement traffic filtering\n",
        "   - Regular security drills and testing\n",
        "   - Forensic data collection capabilities\n",
        "\n",
        "DOCUMENT K3:\n",
        "QoS Configuration Guidelines\n",
        "Quality of Service implementation best practices:\n",
        "1. Classification methods:\n",
        "   - Layer 2: 802.1p CoS (3 bits, values 0-7)\n",
        "   - Layer 3: IP Precedence (3 bits, values 0-7) or DSCP (6 bits, values 0-63)\n",
        "   - Application recognition: NBAR or deep packet inspection\n",
        "2. Queuing mechanisms:\n",
        "   - Priority Queuing (PQ): Strict priority for delay-sensitive traffic\n",
        "   - Weighted Fair Queuing (WFQ): Balanced bandwidth sharing\n",
        "   - Class-Based Weighted Fair Queuing (CBWFQ): Class-based bandwidth allocation\n",
        "   - Low Latency Queuing (LLQ): Combines CBWFQ with priority queue\n",
        "3. Common service classes:\n",
        "   - Voice: EF (DSCP 46), strict priority, minimal delay\n",
        "   - Video: AF41 (DSCP 34), guaranteed bandwidth\n",
        "   - Critical data: AF31 (DSCP 26), guaranteed bandwidth\n",
        "   - Best effort: Default (DSCP 0), remaining bandwidth\n",
        "4. Implementation guidelines:\n",
        "   - Classify and mark traffic as close to the source as possible\n",
        "   - Be consistent with classifications across the network\n",
        "   - Don't assign more than 33% to priority queue to avoid starvation\n",
        "   - Monitor effectiveness with performance metrics\n",
        "\n",
        "# INCIDENT RESOLUTION DOCUMENTS #\n",
        "\n",
        "DOCUMENT I1:\n",
        "Incident Severity Classification and Response\n",
        "Standard incident severity levels and response expectations:\n",
        "1. Critical (Severity 1):\n",
        "   - Definition: Complete outage of a critical business service\n",
        "   - Response time: Immediate (15 minutes or less)\n",
        "   - Resolution goal: 4 hours or less\n",
        "   - Escalation: Automatic to senior engineers and management\n",
        "   - Communication: Hourly updates to stakeholders\n",
        "2. High (Severity 2):\n",
        "   - Definition: Significant degradation of critical service or complete outage of non-critical service\n",
        "   - Response time: 30 minutes or less\n",
        "   - Resolution goal: 8 hours or less\n",
        "   - Escalation: To senior engineers after 2 hours if unresolved\n",
        "   - Communication: Updates every 2 hours\n",
        "3. Medium (Severity 3):\n",
        "   - Definition: Minor service degradation or issue affecting small user group\n",
        "   - Response time: 2 hours or less\n",
        "   - Resolution goal: 24 hours or less\n",
        "   - Escalation: To senior engineers after 8 hours if unresolved\n",
        "   - Communication: Daily updates\n",
        "4. Low (Severity 4):\n",
        "   - Definition: Minimal impact, often a single user or cosmetic issue\n",
        "   - Response time: Next business day\n",
        "   - Resolution goal: Within 5 business days\n",
        "   - Escalation: Based on SLA adherence\n",
        "   - Communication: As needed or requested\n",
        "\n",
        "DOCUMENT I2:\n",
        "Root Cause Analysis Methodology\n",
        "Structured approach to identifying incident root causes:\n",
        "1. Data collection phase:\n",
        "   - Gather all logs and monitoring data\n",
        "   - Document timeline of events\n",
        "   - Collect configuration files and changes\n",
        "   - Interview involved personnel\n",
        "2. Analysis techniques:\n",
        "   - Timeline analysis: Map all events chronologically\n",
        "   - Change analysis: Review recent changes prior to incident\n",
        "   - Correlation analysis: Identify patterns across multiple data sources\n",
        "   - The \"5 Whys\" technique: Repeatedly ask why to drill down to root cause\n",
        "3. Categorization framework:\n",
        "   - Technology issues: Hardware failures, software bugs, capacity limits\n",
        "   - Process issues: Inadequate procedures, missed steps, poor communication\n",
        "   - People issues: Training gaps, human error, resource constraints\n",
        "4. Documentation requirements:\n",
        "   - Incident summary and impact\n",
        "   - Timeline of key events\n",
        "   - Root cause statement (specific, factual, and actionable)\n",
        "   - Contributing factors\n",
        "   - Recommendations to prevent recurrence\n",
        "\n",
        "DOCUMENT I3:\n",
        "Incident Communication Templates\n",
        "Standardized communication formats for effective incident management:\n",
        "1. Initial notification:\n",
        "   - Incident identifier and timestamp\n",
        "   - Brief description of the issue\n",
        "   - Known impact (services and users affected)\n",
        "   - Current status (investigating, identified, in progress)\n",
        "   - Next update timing\n",
        "2. Status update:\n",
        "   - Reference to incident identifier\n",
        "   - Current status and progress\n",
        "   - Actions taken since last update\n",
        "   - Current impact assessment\n",
        "   - Estimated time to resolution (if known)\n",
        "   - Next update timing\n",
        "3. Resolution notification:\n",
        "   - Reference to incident identifier\n",
        "   - Confirmation of resolution\n",
        "   - Brief description of solution implemented\n",
        "   - Current service status\n",
        "   - Duration of incident\n",
        "   - Follow-up activities planned\n",
        "4. Post-incident summary:\n",
        "   - Incident overview and duration\n",
        "   - Root cause and contributing factors\n",
        "   - Resolution steps taken\n",
        "   - Preventive measures identified\n",
        "  - Lessons learned and recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D46ubLACLTX8"
      },
      "source": [
        "### After Executing  all the cells we can now run the Demo to test our AI application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU-Hrq03NPcm",
        "outputId": "e7e96874-a25d-4611-c138-0f35911a3948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-26 06:24:58.917166: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742970298.937667    6670 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742970298.943921    6670 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-26 06:24:58.964593: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-26 06:25:01,654 - gradio_app - INFO - Loading TinyLlama model for LangGraph...\n",
            "2025-03-26 06:25:02,290 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "Device set to use cuda:0\n",
            "/content/agentic_rag-mcp_system/update_gradio_app.py:43: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  return HuggingFacePipeline(pipeline=pipe)\n",
            "2025-03-26 06:25:03,316 - LangGraphDeviceSearchAgent - INFO - LangGraph Device Search Agent initialized\n",
            "2025-03-26 06:25:03,317 - mcp_system - INFO - Registered agent: Troubleshoot-IDP (troubleshooting)\n",
            "2025-03-26 06:25:03,317 - mcp_system - INFO - Registered agent: Device-search-IDP (device_search)\n",
            "2025-03-26 06:25:03,317 - mcp_system - INFO - Registered agent: KnowledgeBase-IDP (knowledge_base)\n",
            "2025-03-26 06:25:03,317 - mcp_system - INFO - Registered agent: Observability-IDP (observability)\n",
            "2025-03-26 06:25:03,317 - mcp_system - INFO - Registered agent: Incident-IDP (incident_resolution)\n",
            "2025-03-26 06:25:03,317 - mcp_system - INFO - RAG system configured for all agents\n",
            "2025-03-26 06:25:03,833 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "2025-03-26 06:25:04,064 - httpx - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
            "2025-03-26 06:25:04,080 - httpx - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
            "2025-03-26 06:25:04,222 - httpx - INFO - HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n",
            "* Running on public URL: https://39050033201b88e120.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "2025-03-26 06:25:16,511 - gradio_app - INFO - Initializing system...\n",
            "2025-03-26 06:25:16,511 - rag_system - INFO - Initializing RAG system...\n",
            "2025-03-26 06:25:16,511 - rag_system - INFO - Test mapping: 'TROUBLESHOOTING' → 'troubleshooting'\n",
            "2025-03-26 06:25:16,511 - rag_system - INFO - Test mapping: 'OBSERVABILITY' → 'observability'\n",
            "2025-03-26 06:25:16,511 - rag_system - INFO - Test mapping: 'DEVICE SEARCH' → 'device_search'\n",
            "2025-03-26 06:25:16,511 - rag_system - INFO - Test mapping: 'KNOWLEDGE BASE' → 'knowledge_base'\n",
            "2025-03-26 06:25:16,511 - rag_system - INFO - Test mapping: 'INCIDENT' → 'incident_resolution'\n",
            "2025-03-26 06:25:16,511 - rag_system - INFO - Test mapping: 'DEVICE INVENTORY' → 'device_inventory'\n",
            "2025-03-26 06:25:16,511 - rag_system - INFO - Loading documents from /content/agentic_rag-mcp_system/network_knowledge_base.txt...\n",
            "2025-03-26 06:25:16,512 - rag_system - INFO - Successfully read knowledge base file, size: 17668 bytes\n",
            "2025-03-26 06:25:16,512 - rag_system - INFO - Found raw section names: ['DEVICE INVENTORY', 'TROUBLESHOOTING', 'OBSERVABILITY', 'DEVICE SEARCH', 'KNOWLEDGE BASE', 'INCIDENT RESOLUTION']\n",
            "2025-03-26 06:25:16,512 - rag_system - INFO - Found 6 sections in knowledge base\n",
            "2025-03-26 06:25:16,512 - rag_system - INFO - Processing section: DEVICE INVENTORY, content length: 2765\n",
            "2025-03-26 06:25:16,512 - rag_system - INFO - Mapped section 'DEVICE INVENTORY' to agent type 'device_inventory'\n",
            "2025-03-26 06:25:16,512 - rag_system - INFO - Loaded 4 documents for device_inventory agent\n",
            "2025-03-26 06:25:16,512 - rag_system - INFO - Sample doc for device_inventory: Core Network Devices\n",
            "- Device ID: R001\n",
            "  Name: core-router-01\n",
            "  Type: router\n",
            "  Status: active\n",
            "  Loca...\n",
            "2025-03-26 06:25:16,512 - rag_system - INFO - Processing section: TROUBLESHOOTING, content length: 2609\n",
            "2025-03-26 06:25:16,512 - rag_system - INFO - Mapped section 'TROUBLESHOOTING' to agent type 'troubleshooting'\n",
            "2025-03-26 06:25:16,512 - rag_system - INFO - Loaded 3 documents for troubleshooting agent\n",
            "2025-03-26 06:25:16,512 - rag_system - INFO - Sample doc for troubleshooting: Router High CPU Troubleshooting Guide\n",
            "When a router shows high CPU utilization (>80%), first identif...\n",
            "2025-03-26 06:25:16,512 - rag_system - INFO - Processing section: OBSERVABILITY, content length: 2432\n",
            "2025-03-26 06:25:16,512 - rag_system - INFO - Mapped section 'OBSERVABILITY' to agent type 'observability'\n",
            "2025-03-26 06:25:16,512 - rag_system - INFO - Loaded 3 documents for observability agent\n",
            "2025-03-26 06:25:16,512 - rag_system - INFO - Sample doc for observability: Network Metric Baselines and Thresholds\n",
            "Understanding normal behavior is essential for effective mon...\n",
            "2025-03-26 06:25:16,513 - rag_system - INFO - Processing section: DEVICE SEARCH, content length: 2938\n",
            "2025-03-26 06:25:16,513 - rag_system - INFO - Mapped section 'DEVICE SEARCH' to agent type 'device_search'\n",
            "2025-03-26 06:25:16,513 - rag_system - INFO - Loaded 3 documents for device_search agent\n",
            "2025-03-26 06:25:16,513 - rag_system - INFO - Sample doc for device_search: Network Topology Fundamentals\n",
            "Understanding dependencies between network devices is crucial for impa...\n",
            "2025-03-26 06:25:16,513 - rag_system - INFO - Processing section: KNOWLEDGE BASE, content length: 3410\n",
            "2025-03-26 06:25:16,513 - rag_system - INFO - Mapped section 'KNOWLEDGE BASE' to agent type 'knowledge_base'\n",
            "2025-03-26 06:25:16,513 - rag_system - INFO - Loaded 3 documents for knowledge_base agent\n",
            "2025-03-26 06:25:16,513 - rag_system - INFO - Sample doc for knowledge_base: Network Protocol Troubleshooting Quick Reference\n",
            "Common protocols and troubleshooting approaches:\n",
            "1....\n",
            "2025-03-26 06:25:16,513 - rag_system - INFO - Processing section: INCIDENT RESOLUTION, content length: 3317\n",
            "2025-03-26 06:25:16,513 - rag_system - INFO - Mapped section 'INCIDENT RESOLUTION' to agent type 'incident_resolution'\n",
            "2025-03-26 06:25:16,513 - rag_system - INFO - Loaded 3 documents for incident_resolution agent\n",
            "2025-03-26 06:25:16,513 - rag_system - INFO - Sample doc for incident_resolution: Incident Severity Classification and Response\n",
            "Standard incident severity levels and response expecta...\n",
            "2025-03-26 06:25:16,513 - rag_system - INFO - Successfully loaded documents for ['device_inventory', 'troubleshooting', 'observability', 'device_search', 'knowledge_base', 'incident_resolution']\n",
            "2025-03-26 06:25:16,513 - rag_system - INFO - Creating vector stores...\n",
            "/content/agentic_rag-mcp_system/rag_system_updated.py:190: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n",
            "2025-03-26 06:25:16,809 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda\n",
            "2025-03-26 06:25:16,809 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
            "2025-03-26 06:25:20,635 - rag_system - INFO - Embeddings validated, dimension: 384\n",
            "2025-03-26 06:25:20,636 - rag_system - INFO - Split document 1 for device_inventory into 2 chunks\n",
            "2025-03-26 06:25:20,636 - rag_system - INFO - Split document 2 for device_inventory into 2 chunks\n",
            "2025-03-26 06:25:20,636 - rag_system - INFO - Split document 3 for device_inventory into 2 chunks\n",
            "2025-03-26 06:25:20,636 - rag_system - INFO - Split document 4 for device_inventory into 2 chunks\n",
            "2025-03-26 06:25:20,636 - rag_system - INFO - Creating vector store for device_inventory with 8 chunks\n",
            "2025-03-26 06:25:20,967 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
            "2025-03-26 06:25:21,243 - rag_system - INFO - Vector store for device_inventory validated\n",
            "2025-03-26 06:25:21,243 - rag_system - INFO - Split document 1 for troubleshooting into 2 chunks\n",
            "2025-03-26 06:25:21,243 - rag_system - INFO - Split document 2 for troubleshooting into 2 chunks\n",
            "2025-03-26 06:25:21,243 - rag_system - INFO - Split document 3 for troubleshooting into 2 chunks\n",
            "2025-03-26 06:25:21,243 - rag_system - INFO - Creating vector store for troubleshooting with 6 chunks\n",
            "2025-03-26 06:25:21,278 - rag_system - INFO - Vector store for troubleshooting validated\n",
            "2025-03-26 06:25:21,278 - rag_system - INFO - Split document 1 for observability into 2 chunks\n",
            "2025-03-26 06:25:21,278 - rag_system - INFO - Split document 2 for observability into 2 chunks\n",
            "2025-03-26 06:25:21,278 - rag_system - INFO - Split document 3 for observability into 2 chunks\n",
            "2025-03-26 06:25:21,278 - rag_system - INFO - Creating vector store for observability with 6 chunks\n",
            "2025-03-26 06:25:21,311 - rag_system - INFO - Vector store for observability validated\n",
            "2025-03-26 06:25:21,312 - rag_system - INFO - Split document 1 for device_search into 2 chunks\n",
            "2025-03-26 06:25:21,312 - rag_system - INFO - Split document 2 for device_search into 3 chunks\n",
            "2025-03-26 06:25:21,312 - rag_system - INFO - Split document 3 for device_search into 3 chunks\n",
            "2025-03-26 06:25:21,312 - rag_system - INFO - Creating vector store for device_search with 8 chunks\n",
            "2025-03-26 06:25:21,347 - rag_system - INFO - Vector store for device_search validated\n",
            "2025-03-26 06:25:21,347 - rag_system - INFO - Split document 1 for knowledge_base into 3 chunks\n",
            "2025-03-26 06:25:21,347 - rag_system - INFO - Split document 2 for knowledge_base into 3 chunks\n",
            "2025-03-26 06:25:21,347 - rag_system - INFO - Split document 3 for knowledge_base into 3 chunks\n",
            "2025-03-26 06:25:21,347 - rag_system - INFO - Creating vector store for knowledge_base with 9 chunks\n",
            "2025-03-26 06:25:21,389 - rag_system - INFO - Vector store for knowledge_base validated\n",
            "2025-03-26 06:25:21,389 - rag_system - INFO - Split document 1 for incident_resolution into 3 chunks\n",
            "2025-03-26 06:25:21,389 - rag_system - INFO - Split document 2 for incident_resolution into 3 chunks\n",
            "2025-03-26 06:25:21,389 - rag_system - INFO - Split document 3 for incident_resolution into 3 chunks\n",
            "2025-03-26 06:25:21,389 - rag_system - INFO - Creating vector store for incident_resolution with 9 chunks\n",
            "2025-03-26 06:25:21,433 - rag_system - INFO - Vector store for incident_resolution validated\n",
            "2025-03-26 06:25:21,433 - rag_system - INFO - Successfully created 6 vector stores: ['device_inventory', 'troubleshooting', 'observability', 'device_search', 'knowledge_base', 'incident_resolution']\n",
            "2025-03-26 06:25:21,433 - rag_system - INFO - Loading language model...\n",
            "2025-03-26 06:25:21,704 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "Device set to use cuda:0\n",
            "2025-03-26 06:25:23,622 - rag_system - INFO - LLM initialized and validated\n",
            "2025-03-26 06:25:23,622 - rag_system - INFO - RAG system successfully initialized\n",
            "2025-03-26 06:25:23,622 - gradio_app - INFO - System initialized successfully\n",
            "2025-03-26 06:25:37,139 - mcp_system - INFO - ObservabilityAgent processing query: I need to analyze metrics for CI types: Analyze CPU and memory utilization on all routers over the last 24 hours., focusing on these metrics: cpu_utilization, latency, memory_utilization, over time range: last_24h\n",
            "2025-03-26 06:25:37,140 - mcp_system - INFO - Parsed metrics query: CI Types=analyze cpu and memory utilization on all routers over the last 24 hours., Metrics=cpu_utilization, Time Range=last_24h\n",
            "2025-03-26 06:25:37,140 - rag_system - INFO - Available vector stores: ['device_inventory', 'troubleshooting', 'observability', 'device_search', 'knowledge_base', 'incident_resolution']\n",
            "2025-03-26 06:25:37,140 - rag_system - INFO - Available document collections: ['device_inventory', 'troubleshooting', 'observability', 'device_search', 'knowledge_base', 'incident_resolution']\n",
            "2025-03-26 06:25:37,140 - rag_system - INFO - RAG query for observability: i need to analyze metrics for ci types: analyze cpu and memory utilization on all routers over the last 24 hours., focusing on these metrics: cpu_utilization, over time range: last_24h\n",
            "2025-03-26 06:25:37,145 - rag_system - INFO - Performing similarity search for observability, k=6\n",
            "2025-03-26 06:25:37,163 - rag_system - INFO - Retrieved 6 documents\n",
            "2025-03-26 06:25:37,163 - rag_system - INFO - Retrieved doc 1: Document 1 for observability\n",
            "2025-03-26 06:25:37,163 - rag_system - INFO - Content preview: Network Metric Baselines and Thresholds\n",
            "Understanding normal behavior is essential for effective mon...\n",
            "2025-03-26 06:25:37,163 - rag_system - INFO - Retrieved doc 2: Document 3 for observability\n",
            "2025-03-26 06:25:37,163 - rag_system - INFO - Content preview: Network Capacity Planning and Trending\n",
            "Proactive capacity management prevents performance issues.\n",
            "Ke...\n",
            "2025-03-26 06:25:37,163 - rag_system - INFO - Retrieved doc 3: Document 1 for observability\n",
            "2025-03-26 06:25:37,163 - rag_system - INFO - Content preview: Seasonal patterns:\n",
            "- Business hours typically show 2-3x higher utilization than off-hours\n",
            "- Month-en...\n",
            "2025-03-26 06:26:18,351 - mcp_system - INFO - KnowledgeBaseAgent processing query: \"How does BGP route selection work?\"\n",
            "2025-03-26 06:26:18,351 - rag_system - INFO - Available vector stores: ['device_inventory', 'troubleshooting', 'observability', 'device_search', 'knowledge_base', 'incident_resolution']\n",
            "2025-03-26 06:26:18,351 - rag_system - INFO - Available document collections: ['device_inventory', 'troubleshooting', 'observability', 'device_search', 'knowledge_base', 'incident_resolution']\n",
            "2025-03-26 06:26:18,351 - rag_system - INFO - RAG query for knowledge_base: \"how does bgp route selection work?\" (context: \"how does bgp route selection work?\"...)\n",
            "2025-03-26 06:26:18,356 - rag_system - INFO - Performing similarity search for knowledge_base, k=6\n",
            "2025-03-26 06:26:18,374 - rag_system - INFO - Retrieved 6 documents\n",
            "2025-03-26 06:26:18,375 - rag_system - INFO - Retrieved doc 1: Document 1 for knowledge_base\n",
            "2025-03-26 06:26:18,375 - rag_system - INFO - Content preview: - Review AS path and route selection\n",
            "   - Common issues: Peer misconfigurations, route flapping, pol...\n",
            "2025-03-26 06:26:18,375 - rag_system - INFO - Retrieved doc 2: Document 1 for knowledge_base\n",
            "2025-03-26 06:26:18,375 - rag_system - INFO - Content preview: - Check for latency or timeouts\n",
            "   - Common issues: Incorrect DNS servers, cache poisoning, zone tra...\n",
            "2025-03-26 06:26:18,375 - rag_system - INFO - Retrieved doc 3: Document 1 for device_search\n",
            "2025-03-26 06:26:18,375 - rag_system - INFO - Content preview: - Uplink dependency: Access switches depend on distribution switches\n",
            "- Routing peer dependency: Rout...\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://39050033201b88e120.gradio.live\n",
            "Exception ignored in atexit callback: <bound method Client.join of <posthog.client.Client object at 0x7a17a2e15a50>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/posthog/client.py\", line 617, in join\n",
            "    consumer.join()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1119, in join\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Run the demo\n",
        "!python update_gradio_app.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}